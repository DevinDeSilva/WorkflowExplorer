{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c988d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Set, Tuple, DefaultDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import rdflib for parsing the RDF graph\n",
    "import rdflib\n",
    "from rdflib import Graph, URIRef, Literal, BNode\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "\n",
    "\n",
    "\n",
    "# --- Utility Functions (from original src.utils) ---\n",
    "# We include these directly to make the script self-contained\n",
    "# and remove external dependencies.\n",
    "\n",
    "\n",
    "def is_inv_rel(rel: str) -> bool:\n",
    "    \"\"\"Check if a relation is an inverse relation.\"\"\"\n",
    "    return rel.endswith(\"#R\")\n",
    "\n",
    "\n",
    "def get_inv_rel(rel: str) -> str:\n",
    "    \"\"\"Get the inverse of a relation, or vice-versa.\"\"\"\n",
    "    if is_inv_rel(rel):\n",
    "        return rel[:-2]  # Remove '#R'\n",
    "    return f\"{rel}#R\"\n",
    "\n",
    "\n",
    "def get_readable_class(cls: str, schema: Optional[Dict[str, Any]] = None) -> str:\n",
    "    \"\"\"Get a readable name for a class.\"\"\"\n",
    "    if schema and cls in schema[\"classes\"] and \"description\" in schema[\"classes\"][cls]:\n",
    "        return schema[\"classes\"][cls][\"description\"]\n",
    "    return cls.split(\".\")[-1]\n",
    "\n",
    "\n",
    "def get_readable_relation(rel: str, schema: Optional[Dict[str, Any]] = None) -> str:\n",
    "    \"\"\"Get a readable name for a relation.\"\"\"\n",
    "    if (\n",
    "        schema\n",
    "        and rel in schema[\"relations\"]\n",
    "        and \"description\" in schema[\"relations\"][rel]\n",
    "    ):\n",
    "        return schema[\"relations\"][rel][\"description\"]\n",
    "    return rel.split(\".\")[-1]\n",
    "\n",
    "\n",
    "def get_reverse_relation(rel: str, schema: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"Get the reverse relation from the schema.\"\"\"\n",
    "    return schema[\"relations\"].get(rel, {}).get(\"reverse\")\n",
    "\n",
    "\n",
    "def get_reverse_readable_relation(rel: str, schema: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"Get the readable name of the reverse relation.\"\"\"\n",
    "    rev_rel = get_reverse_relation(rel, schema)\n",
    "    if rev_rel and rev_rel in schema[\"relations\"]:\n",
    "        return schema[\"relations\"][rev_rel].get(\"description\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_nodes_by_class(\n",
    "    nodes: List[Dict[str, Any]], cls: str, except_nid: Optional[List[int]] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all nodes of a specific class, with optional exceptions.\"\"\"\n",
    "    if except_nid is None:\n",
    "        except_nid = []\n",
    "    return [n for n in nodes if n[\"class\"] == cls and n[\"nid\"] not in except_nid]\n",
    "\n",
    "\n",
    "def get_non_literals(\n",
    "    nodes: List[Dict[str, Any]], except_nid: Optional[Set[int]] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all nodes that are not literals.\"\"\"\n",
    "    if except_nid is None:\n",
    "        except_nid = set()\n",
    "    return [\n",
    "        n\n",
    "        for n in nodes\n",
    "        if n[\"nid\"] not in except_nid and not n[\"class\"].startswith(\"type.\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def legal_class(cls: str) -> bool:\n",
    "    \"\"\"Check if a class is a legal starting point (not a literal).\"\"\"\n",
    "    return not cls.startswith(\"type.\")\n",
    "\n",
    "\n",
    "def legal_relation(rel: str) -> bool:\n",
    "    \"\"\"Placeholder for relation filtering logic, if any.\"\"\"\n",
    "    # You can add logic here to filter out specific relations\n",
    "    return True\n",
    "\n",
    "\n",
    "def graph_query_to_sexpr(*args, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder for the s-expression conversion function.\n",
    "    In a real scenario, you would copy this function's code here.\n",
    "    For this example, we'll return a placeholder string.\n",
    "    \"\"\"\n",
    "    # In your actual use, you would copy the full function definition for\n",
    "    # graph_query_to_sexpr from src.utils.parser\n",
    "    logging.warning(\"Using placeholder function for graph_query_to_sexpr\")\n",
    "    return \"(PlaceholderSExpression)\"\n",
    "\n",
    "\n",
    "def graph_query_to_sparql(*args, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder for the SPARQL conversion function.\n",
    "    In a real scenario, you would copy this function's code here.\n",
    "    For this example, we'll return a placeholder string.\n",
    "    \"\"\"\n",
    "    # In your actual use, you would copy the full function definition for\n",
    "    # graph_query_to_sparql from src.utils.parser\n",
    "    logging.warning(\"Using placeholder function for graph_query_to_sparql\")\n",
    "    return \"SELECT ?x WHERE { ?x ?y ?z . } # (Placeholder SPARQL)\"\n",
    "\n",
    "\n",
    "# --- End of Utility Functions ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99c341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hard-coded literal_map (as it was an external dependency)\n",
    "# This maps schema types to their full XSD/RDF URIs\n",
    "literal_map: Dict[str, str] = {\n",
    "    \"type.string\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "    \"type.text\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "    \"type.datetime\": \"http://www.w3.org/2001/XMLSchema#dateTime\",\n",
    "    \"type.integer\": \"http://www.w3.org/2001/XMLSchema#int\",\n",
    "    \"type.int\": \"http://www.w3.org/2001/XMLSchema#int\",\n",
    "    \"type.float\": \"http://www.w3.org/2001/XMLSchema#float\",\n",
    "    \"type.boolean\": \"http://www.w3.org/2001/XMLSchema#boolean\",\n",
    "}\n",
    "\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Explorer:\n",
    "    \"\"\"\n",
    "    Loads a pre-built RDF graph and its JSON schema to perform\n",
    "    random schema-guided traversals (walks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kg_name: str):\n",
    "        self.kg_name: str = kg_name\n",
    "        self.schema: Optional[Dict[str, Any]] = None\n",
    "        self.schema_dr: Dict[str, Tuple[str, str]] = {}\n",
    "        self.classes: Set[str] = set()\n",
    "\n",
    "        # In-memory representation of the graph and schema\n",
    "        self.out_relations_cls: DefaultDict[str, set] = defaultdict(set)\n",
    "        self.in_relations_cls: DefaultDict[str, set] = defaultdict(set)\n",
    "        self.cls_2_entid: DefaultDict[str, set] = defaultdict(set)\n",
    "        self.entid_2_cls_ent: Dict[str, Dict[str, Any]] = {}\n",
    "        self.literals_by_cls_rel: DefaultDict[Tuple[str, str], set] = defaultdict(set)\n",
    "\n",
    "    def load_graph_and_schema(\n",
    "        self,\n",
    "        schema_fpath: str,\n",
    "        rdf_fpath: str,\n",
    "        processed_fpath: Optional[str] = None,\n",
    "        use_cache: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads the JSON schema and the RDF graph file.\n",
    "        It builds the in-memory representation needed for exploration.\n",
    "\n",
    "        Args:\n",
    "            schema_fpath: Path to the JSON schema file.\n",
    "            rdf_fpath: Path to the RDF graph file (e.g., .nt, .ttl, .rdf).\n",
    "            processed_fpath: Path to a .pkl file for caching the processed data.\n",
    "            use_cache: If True, try to load from processed_fpath if it exists.\n",
    "        \"\"\"\n",
    "\n",
    "        if use_cache and processed_fpath and os.path.exists(processed_fpath):\n",
    "            logger.info(f\"Loading cached processed data from {processed_fpath}\")\n",
    "            with open(processed_fpath, \"rb\") as f:\n",
    "                processed = pickle.load(f)\n",
    "                self.schema = processed[\"schema\"]\n",
    "                self.schema_dr = processed[\"schema_dr\"]\n",
    "                self.classes = processed[\"classes\"]\n",
    "                self.out_relations_cls = processed[\"out_relations_cls\"]\n",
    "                self.in_relations_cls = processed[\"in_relations_cls\"]\n",
    "                self.cls_2_entid = processed[\"cls_2_entid\"]\n",
    "                self.entid_2_cls_ent = processed[\"entid_2_cls_ent\"]\n",
    "                self.literals_by_cls_rel = processed[\"literals_by_cls_rel\"]\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Processing schema from {schema_fpath}\")\n",
    "\n",
    "        # 1. Load Schema\n",
    "        with open(schema_fpath, \"r\") as f:\n",
    "            self.schema = json.load(f)\n",
    "\n",
    "        if not self.schema:\n",
    "            raise ValueError(\"Schema could not be loaded or is empty.\")\n",
    "\n",
    "        self.classes = set(self.schema.get(\"classes\", {}).keys())\n",
    "\n",
    "        for rel, rel_obj in self.schema.get(\"relations\", {}).items():\n",
    "            domain = rel_obj[\"domain\"]\n",
    "            range_ = rel_obj[\"range\"]\n",
    "\n",
    "            self.schema_dr[rel] = (domain, range_)\n",
    "            self.out_relations_cls[domain].add(rel)\n",
    "            self.in_relations_cls[range_].add(rel)\n",
    "\n",
    "        logger.info(f\"Loading RDF graph from {rdf_fpath}...\")\n",
    "\n",
    "        # 2. Load RDF Graph\n",
    "        g = Graph()\n",
    "        try:\n",
    "            g.parse(rdf_fpath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to parse RDF file {rdf_fpath}: {e}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(f\"Graph loaded with {len(g)} triples. Indexing entities...\")\n",
    "\n",
    "        # 3. Build in-memory indexes from the graph\n",
    "\n",
    "        # Get RDFS.label, fall back to a common alt\n",
    "        label_prop = RDFS.label\n",
    "\n",
    "        # Index Entities and their labels\n",
    "        for cls in tqdm(self.classes, desc=\"Indexing entities by class\"):\n",
    "            if cls.startswith(\"type.\"):  # Skip literal types\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                cls_uri = URIRef(cls)\n",
    "                for ent_uri in g.subjects(RDF.type, cls_uri):\n",
    "                    if not isinstance(ent_uri, URIRef):\n",
    "                        continue  # Skip blank nodes\n",
    "\n",
    "                    ent_id_str = str(ent_uri)\n",
    "                    self.cls_2_entid[cls].add(ent_id_str)\n",
    "\n",
    "                    # Get label\n",
    "                    label_lit = g.value(ent_uri, label_prop)\n",
    "                    label_str = (\n",
    "                        str(label_lit) if label_lit else ent_id_str.split(\"/\")[-1]\n",
    "                    )\n",
    "\n",
    "                    self.entid_2_cls_ent[ent_id_str] = {\"class\": cls, \"name\": label_str}\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error indexing class {cls}: {e}\")\n",
    "\n",
    "        # Index Literals by (domain_class, relation)\n",
    "        for rel, (domain, range_) in tqdm(\n",
    "            self.schema_dr.items(), desc=\"Indexing literals\"\n",
    "        ):\n",
    "            if not range_.startswith(\"type.\"):  # Skip non-literal ranges\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                domain_uri = URIRef(domain)\n",
    "                rel_uri = URIRef(rel)\n",
    "\n",
    "                # Find all subjects of the domain type\n",
    "                for s_uri in g.subjects(RDF.type, domain_uri):\n",
    "                    # For each subject, get the literal objects for this relation\n",
    "                    for o_lit in g.objects(s_uri, rel_uri):\n",
    "                        if isinstance(o_lit, Literal):\n",
    "                            self.literals_by_cls_rel[(domain, rel)].add(str(o_lit))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error indexing literals for relation {rel}: {e}\")\n",
    "\n",
    "        logger.info(\"Finished processing graph and schema.\")\n",
    "\n",
    "        # 4. Save to cache if path provided\n",
    "        if use_cache and processed_fpath:\n",
    "            logger.info(f\"Saving processed data to cache at {processed_fpath}\")\n",
    "            try:\n",
    "                with open(processed_fpath, \"wb\") as f:\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"schema\": self.schema,\n",
    "                            \"schema_dr\": self.schema_dr,\n",
    "                            \"classes\": self.classes,\n",
    "                            \"out_relations_cls\": self.out_relations_cls,\n",
    "                            \"in_relations_cls\": self.in_relations_cls,\n",
    "                            \"cls_2_entid\": self.cls_2_entid,\n",
    "                            \"entid_2_cls_ent\": self.entid_2_cls_ent,\n",
    "                            \"literals_by_cls_rel\": self.literals_by_cls_rel,\n",
    "                        },\n",
    "                        f,\n",
    "                        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to write to cache file {processed_fpath}: {e}\")\n",
    "\n",
    "    def explore(\n",
    "        self,\n",
    "        n_walks: int,\n",
    "        edge_lengths: List[int],\n",
    "        max_retries_per_iter: int = 5,\n",
    "        always_ground_classes: bool = False,\n",
    "        always_ground_literals: bool = False,\n",
    "        sexpr_type_constraint: bool = True,\n",
    "        n_per_pattern: int = 1,\n",
    "        use_functions: Optional[List[str]] = None,\n",
    "        max_skip: Optional[int] = None,\n",
    "        verbose: bool = False,\n",
    "        max_retries: Optional[int] = None,\n",
    "        out_dir: Optional[str] = None,\n",
    "        run_id: str = str(int(time.time())),\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Generates a set of random graph queries (walks) based on the loaded schema.\n",
    "        This version does NOT execute the queries.\n",
    "        \"\"\"\n",
    "        edge_dup, walk_dup = 0, 0\n",
    "        iter = 0\n",
    "        walked_sexpr: DefaultDict[str, int] = defaultdict(int)\n",
    "        walked_sexpr_all: Set[str] = set()\n",
    "        walks: List[Dict[str, Any]] = []\n",
    "        fn_counts: DefaultDict[str, int] = defaultdict(int)\n",
    "        edge_counts: DefaultDict[int, int] = defaultdict(int)\n",
    "        node_counts: DefaultDict[int, int] = defaultdict(int)\n",
    "        retained_iter: List[int] = []\n",
    "\n",
    "        # Create output directory\n",
    "        res_dir_fpath: Optional[str] = None\n",
    "        if out_dir is not None:\n",
    "            res_dir_parts = [\"walks\", self.kg_name, \"-\".join(map(str, edge_lengths))]\n",
    "            if use_functions:\n",
    "                res_dir_parts.append(\"-\".join(use_functions))\n",
    "            res_dir_parts.append(run_id)\n",
    "\n",
    "            res_dir_fpath = os.path.join(out_dir, \"_\".join(res_dir_parts))\n",
    "            os.makedirs(res_dir_fpath, exist_ok=True)\n",
    "            logger.info(f\"Saving results to {res_dir_fpath}\")\n",
    "\n",
    "        pbar = tqdm(total=n_walks, desc=\"Exploring\")\n",
    "        start_time = time.time()\n",
    "        retries = 0\n",
    "        retries_n_walks = 0\n",
    "        retries_per_program: List[int] = []\n",
    "        reached_retry_limit = False\n",
    "\n",
    "        while len(walks) < n_walks and (max_skip is None or iter < n_walks + max_skip):\n",
    "            # Retry loop count-out\n",
    "            if len(walks) == retries_n_walks:\n",
    "                retries += 1\n",
    "                if max_retries is not None and retries > max_retries:\n",
    "                    reached_retry_limit = True\n",
    "                    break\n",
    "            else:\n",
    "                retries_per_program.append(retries)\n",
    "                retries = 0\n",
    "                retries_n_walks = len(walks)\n",
    "\n",
    "            if verbose:\n",
    "                logger.info(f\"--- Iter {iter} ---\")\n",
    "\n",
    "            iter += 1\n",
    "            sampled_n_edges = random.choice(edge_lengths)\n",
    "\n",
    "            res = self.generate_graph_query(\n",
    "                n_edges=sampled_n_edges,\n",
    "                max_retries=max_retries_per_iter,\n",
    "                always_ground_literals=always_ground_literals,\n",
    "                always_ground_classes=always_ground_classes,\n",
    "                use_functions=use_functions,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            if res is None:\n",
    "                edge_dup += 1\n",
    "                if verbose:\n",
    "                    logger.info(\n",
    "                        f\"Skipping (generate_graph_query failed) (count={edge_dup})\"\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "            gq, gq_fn, gq_n_groundings = res\n",
    "\n",
    "            sexpr_anon_noid = graph_query_to_sexpr(\n",
    "                gq,\n",
    "                type_constraint=sexpr_type_constraint,\n",
    "                readable=True,\n",
    "                readable_type=\"anon_noid\",\n",
    "            )\n",
    "            sexpr_machine = graph_query_to_sexpr(\n",
    "                gq, type_constraint=sexpr_type_constraint\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                walked_sexpr[sexpr_anon_noid] >= n_per_pattern\n",
    "                or sexpr_machine in walked_sexpr_all\n",
    "            ):\n",
    "                walk_dup += 1\n",
    "                if verbose:\n",
    "                    logger.info(f\"Already walked. Skipping (count={walk_dup})\")\n",
    "                continue\n",
    "\n",
    "            # This is where SPARQL execution (filter_empty, prune_redundant, save_answers)\n",
    "            # was. It has been removed.\n",
    "\n",
    "            walked_sexpr[sexpr_anon_noid] += 1\n",
    "            walked_sexpr_all.add(sexpr_machine)\n",
    "\n",
    "            # Generate all representations\n",
    "            sexpr_anon = graph_query_to_sexpr(\n",
    "                gq,\n",
    "                type_constraint=sexpr_type_constraint,\n",
    "                readable=True,\n",
    "                readable_type=\"anon\",\n",
    "            )\n",
    "            sexpr_anon_rev = graph_query_to_sexpr(\n",
    "                gq,\n",
    "                type_constraint=sexpr_type_constraint,\n",
    "                readable=True,\n",
    "                readable_type=\"anon\",\n",
    "                use_reverse_relations=True,\n",
    "            )\n",
    "            sexpr_machine_rev = graph_query_to_sexpr(\n",
    "                gq,\n",
    "                type_constraint=sexpr_type_constraint,\n",
    "                readable=False,\n",
    "                use_reverse_relations=True,\n",
    "            )\n",
    "            sexpr_label = graph_query_to_sexpr(\n",
    "                gq,\n",
    "                type_constraint=sexpr_type_constraint,\n",
    "                readable=True,\n",
    "                readable_type=\"label\",\n",
    "            )\n",
    "            sexpr_label_rev = graph_query_to_sexpr(\n",
    "                gq,\n",
    "                type_constraint=sexpr_type_constraint,\n",
    "                readable=True,\n",
    "                readable_type=\"label\",\n",
    "                use_reverse_relations=True,\n",
    "            )\n",
    "            sparql_query = graph_query_to_sparql(gq)  # Removed header addition\n",
    "\n",
    "            walks.append(\n",
    "                {\n",
    "                    \"qid\": len(walks),\n",
    "                    \"function\": gq_fn,\n",
    "                    \"num_node\": len(gq[\"nodes\"]),\n",
    "                    \"num_edge\": len(gq[\"edges\"]),\n",
    "                    \"graph_query\": gq,\n",
    "                    \"s_expression_anon\": sexpr_anon,\n",
    "                    \"s_expression_anon-rev\": sexpr_anon_rev,\n",
    "                    \"s_expression_label\": sexpr_label,\n",
    "                    \"s_expression_label-rev\": sexpr_label_rev,\n",
    "                    \"s_expression_machine\": sexpr_machine,\n",
    "                    \"s_expression_machine-rev\": sexpr_machine_rev,\n",
    "                    \"sparql_query\": sparql_query,\n",
    "                    # \"answer\" field is removed\n",
    "                }\n",
    "            )\n",
    "            retained_iter.append(iter)\n",
    "\n",
    "            fn_counts[gq_fn] += 1\n",
    "            edge_counts[len(gq[\"edges\"])] += 1\n",
    "            node_counts[len(gq[\"nodes\"])] += 1\n",
    "\n",
    "            if verbose:\n",
    "                logger.info(json.dumps(walks[-1], indent=2))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        if max_retries is not None and reached_retry_limit:\n",
    "            logger.info(f\"Stopping exploration: reached retry limit ({max_retries})\")\n",
    "        elif n_walks is not None and max_skip is not None and len(walks) < n_walks:\n",
    "            logger.info(\n",
    "                f\"Stopping exploration: reached maximum attempts ({n_walks + max_skip})\"\n",
    "            )\n",
    "\n",
    "        end_time = time.time()\n",
    "        seen_patterns = {k: v for k, v in dict(walked_sexpr).items() if v > 0}\n",
    "\n",
    "        stats = {\n",
    "            \"n_iters\": iter,\n",
    "            \"n_programs\": len(walks),\n",
    "            \"time_taken\": end_time - start_time,\n",
    "            \"retries\": {\n",
    "                \"avg\": sum(retries_per_program) / len(retries_per_program)\n",
    "                if retries_per_program\n",
    "                else 0,\n",
    "                \"max\": max(retries_per_program) if retries_per_program else 0,\n",
    "            },\n",
    "            \"patterns\": {\n",
    "                \"total\": len(seen_patterns),\n",
    "                \"min_programs\": min(seen_patterns.values()) if seen_patterns else 0,\n",
    "                \"max_programs\": max(seen_patterns.values()) if seen_patterns else 0,\n",
    "                \"avg_programs\": sum(seen_patterns.values()) / len(seen_patterns)\n",
    "                if seen_patterns\n",
    "                else 0,\n",
    "            },\n",
    "            \"n_skipped_seen_node-rel_pair\": edge_dup,\n",
    "            \"n_skipped_seen_pattern\": walk_dup,\n",
    "            \"n_skipped_empty_ans\": 0,  # This is 0 because we removed the check\n",
    "            \"n_programs_per_fn\": dict(fn_counts),\n",
    "            \"n_programs_per_node_count\": dict(node_counts),\n",
    "            \"n_programs_per_edge_count\": dict(edge_counts),\n",
    "            \"n_programs_per_pattern\": seen_patterns,\n",
    "            \"retain_iter_count\": retained_iter,\n",
    "        }\n",
    "\n",
    "        if res_dir_fpath:\n",
    "            out_fpath = os.path.join(res_dir_fpath, \"results.json\")\n",
    "            with open(out_fpath, \"w\") as fh:\n",
    "                json.dump(walks, fh, indent=2)\n",
    "            logger.info(f\"Saved walks to {out_fpath}\")\n",
    "\n",
    "            stats_fpath = os.path.join(res_dir_fpath, \"stats.json\")\n",
    "            with open(stats_fpath, \"w\") as fh:\n",
    "                json.dump(stats, fh, indent=2)\n",
    "            logger.info(f\"Saved exploration statistics to {stats_fpath}\")\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def generate_graph_query(\n",
    "        self,\n",
    "        n_edges: int,\n",
    "        max_retries: int = 3,\n",
    "        always_ground_literals: bool = True,\n",
    "        always_ground_classes: bool = False,\n",
    "        verbose: bool = False,\n",
    "        use_functions: Optional[List[str]] = None,\n",
    "        ground_attempts_max: int = 5,\n",
    "    ) -> Optional[Tuple[Dict[str, List], str, int]]:\n",
    "        \"\"\"\n",
    "        Generates a single random graph query.\n",
    "        This function is largely unchanged, as it depends on the in-memory\n",
    "        schema representation, which is now populated by load_graph_and_schema.\n",
    "        \"\"\"\n",
    "        if not self.schema or not self.classes:\n",
    "            logger.error(\"Schema not loaded. Call load_graph_and_schema() first.\")\n",
    "            return None\n",
    "\n",
    "        graph: Dict[str, List] = {\"nodes\": [], \"edges\": []}\n",
    "        class_2_nid: DefaultDict[str, Set[int]] = defaultdict(set)\n",
    "        sampled_node_rel: Set[Tuple[int, str]] = set()\n",
    "        nodes_2_ground: Set[int] = set()\n",
    "        ungrounded_terminal_node: Set[int] = set()\n",
    "        n_nodes_grounded = 0\n",
    "\n",
    "        # Sample an initial question node from all non-literal classes\n",
    "        legal_classes = [c for c in self.classes if legal_class(c)]\n",
    "        if not legal_classes:\n",
    "            logger.error(\"No legal (non-literal) classes found in schema.\")\n",
    "            return None\n",
    "\n",
    "        q_class = random.choice(legal_classes)\n",
    "\n",
    "        q_node: Dict[str, Any] = {\n",
    "            \"nid\": 0,\n",
    "            \"node_type\": \"class\",\n",
    "            \"id\": q_class,\n",
    "            \"class\": q_class,\n",
    "            \"readable_name\": get_readable_class(q_class, schema=self.schema),\n",
    "            \"question_node\": 1,\n",
    "            \"function\": \"none\",\n",
    "        }\n",
    "        graph[\"nodes\"].append(q_node)\n",
    "        class_2_nid[q_node[\"class\"]].add(q_node[\"nid\"])\n",
    "\n",
    "        n_attempts = 0\n",
    "        n_legal_relation_attempts = 0\n",
    "        while (\n",
    "            len(graph[\"edges\"]) < n_edges\n",
    "            and n_attempts < max_retries\n",
    "            and n_legal_relation_attempts < max_retries\n",
    "        ):\n",
    "            next_edge: Dict[str, Any] = {}\n",
    "            # Sample a node from the set of ungrounded and non-literal nodes\n",
    "            non_literal_nodes = get_non_literals(\n",
    "                graph[\"nodes\"], except_nid=nodes_2_ground\n",
    "            )\n",
    "            if not non_literal_nodes:\n",
    "                if verbose:\n",
    "                    logger.info(\n",
    "                        \"No more non-literal nodes to expand. Stopping walk early.\"\n",
    "                    )\n",
    "                break  # Can't expand anymore\n",
    "\n",
    "            node_2_expand = random.choice(non_literal_nodes)\n",
    "\n",
    "            # Sample adjacent node relation\n",
    "            possible_rels = list(self.out_relations_cls[node_2_expand[\"class\"]]) + [\n",
    "                f\"{r}#R\" for r in self.in_relations_cls[node_2_expand[\"class\"]]\n",
    "            ]\n",
    "\n",
    "            if not possible_rels:\n",
    "                n_attempts += 1\n",
    "                if verbose:\n",
    "                    logger.info(\n",
    "                        f\"Node {node_2_expand['nid']} has no relations. Retrying ({n_attempts})\"\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "            next_rel = random.choice(possible_rels)\n",
    "\n",
    "            if not legal_relation(next_rel):\n",
    "                n_legal_relation_attempts += 1\n",
    "                if verbose:\n",
    "                    logger.info(\n",
    "                        f\"Illegal relation sampled. Retrying ({n_legal_relation_attempts})\"\n",
    "                    )\n",
    "                continue\n",
    "            n_legal_relation_attempts = 0\n",
    "\n",
    "            # Re-try if the sampled (node, relation) pair has been sampled before\n",
    "            if (node_2_expand[\"nid\"], next_rel) in sampled_node_rel:\n",
    "                n_attempts += 1\n",
    "                if verbose:\n",
    "                    logger.info(f\"Already seen. Retrying ({n_attempts})\")\n",
    "                continue\n",
    "            sampled_node_rel.add((node_2_expand[\"nid\"], next_rel))\n",
    "            sampled_node_rel.add((node_2_expand[\"nid\"], get_inv_rel(next_rel)))\n",
    "            n_attempts = 0\n",
    "\n",
    "            ungrounded_terminal_node.discard(node_2_expand[\"nid\"])\n",
    "\n",
    "            if is_inv_rel(next_rel):\n",
    "                rel_name = get_inv_rel(next_rel)\n",
    "                rel_domain, rel_range = (\n",
    "                    self.schema[\"relations\"][rel_name][\"range\"],\n",
    "                    self.schema[\"relations\"][rel_name][\"domain\"],\n",
    "                )\n",
    "            else:\n",
    "                rel_name = next_rel\n",
    "                rel_domain, rel_range = (\n",
    "                    self.schema[\"relations\"][rel_name][\"domain\"],\n",
    "                    self.schema[\"relations\"][rel_name][\"range\"],\n",
    "                )\n",
    "\n",
    "            # Select next node\n",
    "            cand_nodes_from_existing = get_nodes_by_class(\n",
    "                graph[\"nodes\"], cls=rel_range, except_nid=[node_2_expand[\"nid\"]]\n",
    "            )\n",
    "            new_node: Dict[str, Any] = {\n",
    "                \"nid\": len(graph[\"nodes\"]),\n",
    "                \"node_type\": \"class\",\n",
    "                \"readable_name\": get_readable_class(rel_range, schema=self.schema),\n",
    "                \"question_node\": 0,\n",
    "                \"function\": \"none\",\n",
    "                \"id\": rel_range,\n",
    "                \"class\": rel_range,\n",
    "            }\n",
    "\n",
    "            if not cand_nodes_from_existing:\n",
    "                add_new_node = True\n",
    "                next_node = new_node\n",
    "            else:\n",
    "                if add_new_node := random.choice([True, False]):\n",
    "                    next_node = new_node\n",
    "                else:\n",
    "                    next_node = random.choice(cand_nodes_from_existing)\n",
    "\n",
    "            if is_inv_rel(next_rel):\n",
    "                next_edge.update(\n",
    "                    {\n",
    "                        \"start\": next_node[\"nid\"],\n",
    "                        \"end\": node_2_expand[\"nid\"],\n",
    "                        \"relation\": rel_name,\n",
    "                        \"readable_name\": get_readable_relation(\n",
    "                            rel_name, schema=self.schema\n",
    "                        ),\n",
    "                        \"reverse_relation\": get_reverse_relation(\n",
    "                            rel_name, schema=self.schema\n",
    "                        ),\n",
    "                        \"reverse_readable_name\": get_reverse_readable_relation(\n",
    "                            rel_name, schema=self.schema\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                next_edge.update(\n",
    "                    {\n",
    "                        \"start\": node_2_expand[\"nid\"],\n",
    "                        \"end\": next_node[\"nid\"],\n",
    "                        \"relation\": rel_name,\n",
    "                        \"readable_name\": get_readable_relation(\n",
    "                            rel_name, schema=self.schema\n",
    "                        ),\n",
    "                        \"reverse_relation\": get_reverse_relation(\n",
    "                            rel_name, schema=self.schema\n",
    "                        ),\n",
    "                        \"reverse_readable_name\": get_reverse_readable_relation(\n",
    "                            rel_name, schema=self.schema\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if add_new_node:\n",
    "                if next_node[\"class\"].startswith(\"type.\"):  # Check for literal\n",
    "                    if always_ground_literals:\n",
    "                        next_node[\"node_type\"] = \"literal\"\n",
    "                        grounding_cands = self.literals_by_cls_rel[\n",
    "                            (node_2_expand[\"class\"], next_edge[\"relation\"])\n",
    "                        ]\n",
    "                        if not grounding_cands:\n",
    "                            if verbose:\n",
    "                                logger.info(\n",
    "                                    f\"No grounding values found for class `{node_2_expand['class']}` + relation `{next_edge['relation']}`. Skipping.\"\n",
    "                                )\n",
    "                            return None\n",
    "                        next_node[\"readable_name\"] = random.choice(\n",
    "                            list(grounding_cands)\n",
    "                        )\n",
    "                        next_node[\"id\"] = (\n",
    "                            f'\"{next_node[\"readable_name\"]}\"^^{literal_map[next_node[\"class\"]]}'\n",
    "                        )\n",
    "                        n_nodes_grounded += 1\n",
    "                    else:\n",
    "                        next_node[\"grounding_helper\"] = {\n",
    "                            \"cls\": node_2_expand[\"class\"],\n",
    "                            \"rel\": next_edge[\"relation\"],\n",
    "                        }\n",
    "                        if random.choice([True, False]):\n",
    "                            nodes_2_ground.add(next_node[\"nid\"])\n",
    "                else:  # non-literal class\n",
    "                    if always_ground_classes or random.choice([True, False]):\n",
    "                        if len(self.cls_2_entid[next_node[\"class\"]]) > 0:\n",
    "                            nodes_2_ground.add(next_node[\"nid\"])\n",
    "\n",
    "                graph[\"nodes\"].append(next_node)\n",
    "                class_2_nid[next_node[\"class\"]].add(next_node[\"nid\"])\n",
    "                if (\n",
    "                    next_node[\"node_type\"] == \"class\"\n",
    "                    and next_node[\"nid\"] not in nodes_2_ground\n",
    "                ):\n",
    "                    ungrounded_terminal_node.add(next_node[\"nid\"])\n",
    "\n",
    "            graph[\"edges\"].append(next_edge)\n",
    "            sampled_node_rel.add((next_node[\"nid\"], next_rel))\n",
    "            sampled_node_rel.add((next_node[\"nid\"], get_inv_rel(next_rel)))\n",
    "\n",
    "        if len(graph[\"edges\"]) == 0 and n_edges > 0:\n",
    "            if verbose:\n",
    "                logger.info(\"Could not sample any edges.\")\n",
    "            return None\n",
    "\n",
    "        if len(graph[\"edges\"]) < n_edges and n_attempts >= max_retries:\n",
    "            if verbose:\n",
    "                logger.info(\n",
    "                    \"Reached maximum attempts in trying to sample new node-edge pairs\"\n",
    "                )\n",
    "            return None\n",
    "\n",
    "        # Mark ungrounded terminal nodes for grounding\n",
    "        nodes_2_ground.update(ungrounded_terminal_node)\n",
    "\n",
    "        # Add a function to the query\n",
    "        function_cands = [\"none\"]\n",
    "        if not q_class.startswith(\"type.\"):\n",
    "            function_cands.append(\"count\")\n",
    "\n",
    "        numerical_nodes_not_q = class_2_nid[\"type.integer\"].union(\n",
    "            class_2_nid[\"type.float\"]\n",
    "        ).union(class_2_nid[\"type.datetime\"]) - {q_node[\"nid\"]}\n",
    "\n",
    "        if len(numerical_nodes_not_q) > 0:\n",
    "            function_cands.extend([\"argmax\", \"argmin\", \">\", \"<\", \">=\", \"<=\"])\n",
    "        else:\n",
    "            function_cands += [\"none\"] * 2  # reduce likelihood of 'count'\n",
    "\n",
    "        sampled_fn = random.choice(function_cands)\n",
    "\n",
    "        if use_functions is not None and sampled_fn not in use_functions:\n",
    "            return None\n",
    "\n",
    "        if sampled_fn == \"count\":\n",
    "            graph[\"nodes\"][0][\"function\"] = \"count\"\n",
    "        elif sampled_fn in [\"argmin\", \"argmax\"]:\n",
    "            sampled_node_id = random.choice(list(numerical_nodes_not_q))\n",
    "            graph[\"nodes\"][sampled_node_id].update(\n",
    "                {\n",
    "                    \"node_type\": \"literal\",\n",
    "                    \"id\": '\"0\"^^http://www.w3.org/2001/XMLSchema#int',\n",
    "                    \"readable_name\": \"0\",\n",
    "                    \"function\": sampled_fn,\n",
    "                }\n",
    "            )\n",
    "            nodes_2_ground -= {sampled_node_id}\n",
    "            n_nodes_grounded += 1\n",
    "        elif sampled_fn in [\">\", \"<\", \">=\", \"<=\"]:\n",
    "            sampled_node_id = random.choice(list(numerical_nodes_not_q))\n",
    "            graph[\"nodes\"][sampled_node_id][\"function\"] = sampled_fn\n",
    "            if graph[\"nodes\"][sampled_node_id][\"node_type\"] == \"class\":\n",
    "                nodes_2_ground.add(sampled_node_id)\n",
    "\n",
    "        # Try to ground something if nothing is grounded so far\n",
    "        if n_nodes_grounded == 0:\n",
    "            ground_attempts = 0\n",
    "            while len(nodes_2_ground) == 0 and ground_attempts < ground_attempts_max:\n",
    "                sampled_node = random.choice(graph[\"nodes\"][1:])  # Don't ground q_node\n",
    "                if sampled_node[\"node_type\"] == \"class\":\n",
    "                    nodes_2_ground.add(sampled_node[\"nid\"])\n",
    "                ground_attempts += 1\n",
    "\n",
    "        if (\n",
    "            n_nodes_grounded == 0\n",
    "            and len(nodes_2_ground) == 0\n",
    "            and len(graph[\"nodes\"]) > 1\n",
    "        ):\n",
    "            # As a last resort, if we still have no groundings,\n",
    "            # try to ground *any* non-question node\n",
    "            non_q_nodes = [\n",
    "                n for n in graph[\"nodes\"] if n[\"nid\"] != 0 and n[\"node_type\"] == \"class\"\n",
    "            ]\n",
    "            if non_q_nodes:\n",
    "                nodes_2_ground.add(random.choice(non_q_nodes)[\"nid\"])\n",
    "\n",
    "        # Ground nodes\n",
    "        grounded_ents: Set[str] = set()\n",
    "        for nid in nodes_2_ground:\n",
    "            node = graph[\"nodes\"][nid]\n",
    "            node_cls = node[\"class\"]\n",
    "\n",
    "            if node_cls.startswith(\"type.\"):  # literal class\n",
    "                g_helper = node.get(\"grounding_helper\")\n",
    "                if not g_helper:\n",
    "                    if verbose:\n",
    "                        logger.info(\n",
    "                            f\"Node {nid} marked for grounding but has no grounding_helper. Skipping.\"\n",
    "                        )\n",
    "                    continue\n",
    "\n",
    "                grounding_cands = list(\n",
    "                    self.literals_by_cls_rel[(g_helper[\"cls\"], g_helper[\"rel\"])]\n",
    "                )\n",
    "                if not grounding_cands:\n",
    "                    if verbose:\n",
    "                        logger.warning(\n",
    "                            f\"No grounding values found for `{g_helper['cls']}->{g_helper['rel']}`. Cannot ground.\"\n",
    "                        )\n",
    "                    return None  # This walk is invalid\n",
    "\n",
    "                grounded_lit = random.choice(grounding_cands)\n",
    "                node.update(\n",
    "                    {\n",
    "                        \"node_type\": \"literal\",\n",
    "                        \"id\": f'\"{grounded_lit}\"^^{literal_map[node_cls]}',\n",
    "                        \"readable_name\": grounded_lit,\n",
    "                    }\n",
    "                )\n",
    "            else:  # entity class\n",
    "                if not self.cls_2_entid[node_cls]:\n",
    "                    if verbose:\n",
    "                        logger.info(\n",
    "                            f\"No entities found for class {node_cls}. Skipping.\"\n",
    "                        )\n",
    "                    return None\n",
    "\n",
    "                grounding_cands = list(self.cls_2_entid[node_cls] - grounded_ents)\n",
    "\n",
    "                if not grounding_cands:\n",
    "                    # All entities for this class are already used. Use one again.\n",
    "                    grounding_cands = list(self.cls_2_entid[node_cls])\n",
    "                    if not grounding_cands:\n",
    "                        if verbose:\n",
    "                            logger.info(\n",
    "                                f\"No grounding values found for class `{node_cls}`. Skipping\"\n",
    "                            )\n",
    "                        return None\n",
    "\n",
    "                grounded_ent = random.choice(grounding_cands)\n",
    "                grounded_ents.add(grounded_ent)\n",
    "                node.update(\n",
    "                    {\n",
    "                        \"node_type\": \"entity\",\n",
    "                        \"id\": grounded_ent,\n",
    "                        \"readable_name\": self.entid_2_cls_ent[grounded_ent][\"name\"],\n",
    "                    }\n",
    "                )\n",
    "            n_nodes_grounded += 1\n",
    "\n",
    "        if n_nodes_grounded == 0 and len(graph[\"nodes\"]) > 1:\n",
    "            if verbose:\n",
    "                logger.info(\"Failed to ground any node. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        return graph, sampled_fn, n_nodes_grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e67f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_explorer = Explorer(kg_name=\"workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6935fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 14:09:19,653 - INFO - Processing schema from data/graphs/workflow/schema.json\n",
      "2025-11-05 14:09:19,654 - INFO - Loading RDF graph from data/graphs/workflow/chatbs_sample.ttl...\n",
      "2025-11-05 14:09:19,673 - INFO - Graph loaded with 1053 triples. Indexing entities...\n",
      "Indexing entities by class: 100%|██████████| 15/15 [00:00<00:00, 52254.62it/s]\n",
      "Indexing literals: 100%|██████████| 23/23 [00:00<00:00, 249273.88it/s]\n",
      "2025-11-05 14:09:20,386 - INFO - Finished processing graph and schema.\n",
      "2025-11-05 14:09:20,386 - INFO - Saving processed data to cache at data/graphs/workflow/processed_workflow.pkl\n"
     ]
    }
   ],
   "source": [
    "workflow_explorer.load_graph_and_schema(\n",
    "    schema_fpath=\"data/graphs/workflow/schema.json\",\n",
    "    rdf_fpath=\"data/graphs/workflow/chatbs_sample.ttl\",\n",
    "    processed_fpath=\"data/graphs/workflow/processed_workflow.pkl\",\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a798d8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'provone.hasSubProgram': ('provone.Program', 'provone.Program'),\n",
       " 'provone.controls': ('provone.Controller', 'provone.Program'),\n",
       " 'provone.hasInPort': ('provone.Program', 'provone.Port'),\n",
       " 'provone.hasOutPort': ('provone.Program', 'provone.Port'),\n",
       " 'provone.hasDefaultParam': ('provone.Port', 'prov.Entity'),\n",
       " 'provone.connectsTo': ('provone.Port', 'provone.Channel'),\n",
       " 'provone.programWasDerivedFrom': ('provone.Program', 'provone.Program'),\n",
       " 'prov.used': ('provone.Execution', 'prov.Entity'),\n",
       " 'prov.wasGeneratedBy': ('prov.Entity', 'provone.Execution'),\n",
       " 'prov.wasAssociatedWith': ('provone.Execution', 'provone.User'),\n",
       " 'prov.wasInformedBy': ('provone.Execution', 'provone.Execution'),\n",
       " 'provone.wasPartOf': ('provone.Execution', 'provone.Execution'),\n",
       " 'prov.qualifiedAssociation': ('provone.Execution', 'prov.Association'),\n",
       " 'prov.agent': ('prov.Association', 'provone.User'),\n",
       " 'prov.hadPlan': ('prov.Association', 'provone.Program'),\n",
       " 'prov.qualifiedUsage': ('provone.Execution', 'prov.Usage'),\n",
       " 'provone.usageHadInPort': ('prov.Usage', 'provone.Port'),\n",
       " 'provone.usageHadEntity': ('prov.Usage', 'prov.Entity'),\n",
       " 'prov.qualifiedGeneration': ('provone.Execution', 'prov.Generation'),\n",
       " 'provone.generationHadOutPort': ('prov.Generation', 'provone.Port'),\n",
       " 'provone.generationHadEntity': ('prov.Generation', 'prov.Entity'),\n",
       " 'provone.dataWasDerivedFrom': ('prov.Entity', 'prov.Entity'),\n",
       " 'prov.hadMember': ('prov.Collection', 'prov.Entity')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_explorer.schema_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b37c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': {'provone.Program': {'description': 'A computational task that consumes and produces data. Can be atomic or composite.'},\n",
       "  'provone.Workflow': {'description': 'A distinguished Program, representing a computational experiment in its entirety. Subclass of Program.'},\n",
       "  'provone.Port': {'description': 'Enables a Program to send or receive Entity items (Data, Visualization, or Document instances).'},\n",
       "  'provone.Channel': {'description': 'Provides a connection between Ports that are defined for Programs.'},\n",
       "  'provone.Controller': {'description': 'Specifies a Program that controls other Programs under a particular model of computation.'},\n",
       "  'provone.Execution': {'description': 'Represents the execution of a Program. If the Program is a Workflow, this is the trace.'},\n",
       "  'provone.User': {'description': 'The person(s) responsible for the execution of an Execution. Subclass of prov:Agent.'},\n",
       "  'prov.Entity': {'description': 'A physical, digital, conceptual, or other kind of thing. Base class for data.'},\n",
       "  'provone.Data': {'description': 'Represents the basic unit of information consumed or produced by a Program. Subclass of prov:Entity.'},\n",
       "  'provone.Visualization': {'description': 'A digital visual representation consumed or produced by a Program. Subclass of prov:Entity.'},\n",
       "  'provone.Document': {'description': 'A body of information (e.g., article, report) produced as a result of an Execution. Subclass of prov:Entity.'},\n",
       "  'prov.Collection': {'description': 'An entity that provides a structure to some constituents, which are themselves entities.'},\n",
       "  'prov.Association': {'description': 'An assignment of responsibility to an agent (User) for an activity (Execution), specifying a plan (Program).'},\n",
       "  'prov.Usage': {'description': \"The beginning of utilizing an entity by an activity (Execution). Qualifies the 'used' relation.\"},\n",
       "  'prov.Generation': {'description': \"The completion of production of a new entity by an activity (Execution). Qualifies the 'wasGeneratedBy' relation.\"}},\n",
       " 'relations': {'provone.hasSubProgram': {'description': 'Specifies the recursive composition of Programs, linking a parent Program to a child Program.',\n",
       "   'domain': 'provone.Program',\n",
       "   'range': 'provone.Program',\n",
       "   'reverse': 'provone.isSubProgramOf',\n",
       "   'reverse_description': 'The parent Program this Program is a part of.'},\n",
       "  'provone.controls': {'description': 'Relates a Controller to its destination Program.',\n",
       "   'domain': 'provone.Controller',\n",
       "   'range': 'provone.Program',\n",
       "   'reverse': 'provone.controlledBy',\n",
       "   'reverse_description': 'Relates a Program to the Controller that controls it.'},\n",
       "  'provone.hasInPort': {'description': 'Specifies the Ports of a particular Program that are used as input ports.',\n",
       "   'domain': 'provone.Program',\n",
       "   'range': 'provone.Port',\n",
       "   'reverse': 'provone.isInputPortOf',\n",
       "   'reverse_description': 'The Program this Port serves as an input for.'},\n",
       "  'provone.hasOutPort': {'description': 'Specifies the Ports of a particular Program that are used as output ports.',\n",
       "   'domain': 'provone.Program',\n",
       "   'range': 'provone.Port',\n",
       "   'reverse': 'provone.isOutputPortOf',\n",
       "   'reverse_description': 'The Program this Port serves as an output for.'},\n",
       "  'provone.hasDefaultParam': {'description': 'Specifies that a given input Port has a certain Entity item as a default parameter.',\n",
       "   'domain': 'provone.Port',\n",
       "   'range': 'prov.Entity',\n",
       "   'reverse': 'provone.isDefaultParamFor',\n",
       "   'reverse_description': 'The Port(s) this Entity serves as a default parameter for.'},\n",
       "  'provone.connectsTo': {'description': 'Specifies the Channel that the given Port(s) connect to.',\n",
       "   'domain': 'provone.Port',\n",
       "   'range': 'provone.Channel',\n",
       "   'reverse': 'provone.connectsPort',\n",
       "   'reverse_description': 'The Port(s) that this Channel connects.'},\n",
       "  'provone.programWasDerivedFrom': {'description': 'Describes the evolution of programs and workflows, linking a new version to an old one.',\n",
       "   'domain': 'provone.Program',\n",
       "   'range': 'provone.Program',\n",
       "   'reverse': 'provone.programHadDerivation',\n",
       "   'reverse_description': 'Program(s) that were derived from this program.'},\n",
       "  'prov.used': {'description': 'States that an Execution made use of a particular Entity item as input.',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'prov.Entity',\n",
       "   'reverse': 'prov.wasUsedBy',\n",
       "   'reverse_description': 'The Execution(s) that used this Entity.'},\n",
       "  'prov.wasGeneratedBy': {'description': 'States that an Entity item was produced as an output of an Execution.',\n",
       "   'domain': 'prov.Entity',\n",
       "   'range': 'provone.Execution',\n",
       "   'reverse': 'prov.generated',\n",
       "   'reverse_description': 'The Entity/Entities generated by this Execution.'},\n",
       "  'prov.wasAssociatedWith': {'description': 'States that a User was associated with a particular Execution (assignment of attribution).',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'provone.User',\n",
       "   'reverse': 'prov.hadAssociationWith',\n",
       "   'reverse_description': 'The Execution(s) this User was associated with.'},\n",
       "  'prov.wasInformedBy': {'description': 'States that an Execution communicates with and is informed by another Execution.',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'provone.Execution',\n",
       "   'reverse': 'prov.informed',\n",
       "   'reverse_description': 'The Execution(s) that this Execution informed.'},\n",
       "  'provone.wasPartOf': {'description': 'Links a child Execution (of a sub-program) to its parent Execution (of a workflow).',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'provone.Execution',\n",
       "   'reverse': 'provone.hadPart',\n",
       "   'reverse_description': 'Child Execution(s) that are part of this parent Execution.'},\n",
       "  'prov.qualifiedAssociation': {'description': 'Links an Execution to a detailed Association object, which specifies the agent (User) and plan (Program).',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'prov.Association',\n",
       "   'reverse': 'prov.associationOf',\n",
       "   'reverse_description': 'The Execution this Association qualifies.'},\n",
       "  'prov.agent': {'description': 'Links an Association to the agent (User) responsible.',\n",
       "   'domain': 'prov.Association',\n",
       "   'range': 'provone.User',\n",
       "   'reverse': 'prov.wasAgentFor',\n",
       "   'reverse_description': 'The Association(s) in which this User acted as the agent.'},\n",
       "  'prov.hadPlan': {'description': 'Links an Association to the Program or Workflow that was executed.',\n",
       "   'domain': 'prov.Association',\n",
       "   'range': 'provone.Program',\n",
       "   'reverse': 'prov.wasPlanFor',\n",
       "   'reverse_description': 'The Association(s) that used this Program as a Plan.'},\n",
       "  'prov.qualifiedUsage': {'description': 'Links an Execution to a detailed Usage object, which qualifies how an Entity was used.',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'prov.Usage',\n",
       "   'reverse': 'prov.usageOf',\n",
       "   'reverse_description': 'The Execution this Usage event qualifies.'},\n",
       "  'provone.usageHadInPort': {'description': 'Specifies the input Port used in a given Usage event.',\n",
       "   'domain': 'prov.Usage',\n",
       "   'range': 'provone.Port',\n",
       "   'reverse': 'provone.wasInputPortInUsage',\n",
       "   'reverse_description': 'The Usage event(s) associated with this input Port.'},\n",
       "  'provone.usageHadEntity': {'description': 'Specifies the Entity used in a given Usage event.',\n",
       "   'domain': 'prov.Usage',\n",
       "   'range': 'prov.Entity',\n",
       "   'reverse': 'provone.wasEntityInUsage',\n",
       "   'reverse_description': 'The Usage event(s) in which this Entity was used.'},\n",
       "  'prov.qualifiedGeneration': {'description': 'Links an Execution to a detailed Generation object, which qualifies how an Entity was generated.',\n",
       "   'domain': 'provone.Execution',\n",
       "   'range': 'prov.Generation',\n",
       "   'reverse': 'prov.generationOf',\n",
       "   'reverse_description': 'The Execution this Generation event qualifies.'},\n",
       "  'provone.generationHadOutPort': {'description': 'Specifies the output Port used in a given Generation event.',\n",
       "   'domain': 'prov.Generation',\n",
       "   'range': 'provone.Port',\n",
       "   'reverse': 'provone.wasOutputPortInGeneration',\n",
       "   'reverse_description': 'The Generation event(s) associated with this output Port.'},\n",
       "  'provone.generationHadEntity': {'description': 'Specifies the Entity generated in a given Generation event.',\n",
       "   'domain': 'prov.Generation',\n",
       "   'range': 'prov.Entity',\n",
       "   'reverse': 'provone.wasEntityInGeneration',\n",
       "   'reverse_description': 'The Generation event(s) in which this Entity was generated.'},\n",
       "  'provone.dataWasDerivedFrom': {'description': 'Describes dependencies between data items produced during workflow execution.',\n",
       "   'domain': 'prov.Entity',\n",
       "   'range': 'prov.Entity',\n",
       "   'reverse': 'provone.dataHadDerivation',\n",
       "   'reverse_description': 'Entity/Entities that were derived from this Entity.'},\n",
       "  'prov.hadMember': {'description': 'Specifies the Entity items that form part of a Collection.',\n",
       "   'domain': 'prov.Collection',\n",
       "   'range': 'prov.Entity',\n",
       "   'reverse': 'prov.isMemberOf',\n",
       "   'reverse_description': 'The Collection(s) this Entity is a member of.'}},\n",
       " 'inverse_relations': {'provone.isSubProgramOf': 'provone.hasSubProgram',\n",
       "  'provone.controlledBy': 'provone.controls',\n",
       "  'provone.isInputPortOf': 'provone.hasInPort',\n",
       "  'provone.isOutputPortOf': 'provone.hasOutPort',\n",
       "  'provone.isDefaultParamFor': 'provone.hasDefaultParam',\n",
       "  'provone.connectsPort': 'provone.connectsTo',\n",
       "  'provone.programHadDerivation': 'provone.programWasDerivedFrom',\n",
       "  'prov.wasUsedBy': 'prov.used',\n",
       "  'prov.generated': 'prov.wasGeneratedBy',\n",
       "  'prov.hadAssociationWith': 'prov.wasAssociatedWith',\n",
       "  'prov.informed': 'prov.wasInformedBy',\n",
       "  'provone.hadPart': 'provone.wasPartOf',\n",
       "  'prov.associationOf': 'prov.qualifiedAssociation',\n",
       "  'prov.wasAgentFor': 'prov.agent',\n",
       "  'prov.wasPlanFor': 'prov.hadPlan',\n",
       "  'prov.usageOf': 'prov.qualifiedUsage',\n",
       "  'provone.wasInputPortInUsage': 'provone.usageHadInPort',\n",
       "  'provone.wasEntityInUsage': 'provone.usageHadEntity',\n",
       "  'prov.generationOf': 'prov.qualifiedGeneration',\n",
       "  'provone.wasOutputPortInGeneration': 'provone.generationHadOutPort',\n",
       "  'provone.wasEntityInGeneration': 'provone.generationHadEntity',\n",
       "  'provone.dataHadDerivation': 'provone.dataWasDerivedFrom',\n",
       "  'prov.isMemberOf': 'prov.hadMember'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_explorer.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643964af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'provone.Program': {'provone.hasInPort',\n",
       "              'provone.hasOutPort',\n",
       "              'provone.hasSubProgram',\n",
       "              'provone.programWasDerivedFrom'},\n",
       "             'provone.Controller': {'provone.controls'},\n",
       "             'provone.Port': {'provone.connectsTo', 'provone.hasDefaultParam'},\n",
       "             'provone.Execution': {'prov.qualifiedAssociation',\n",
       "              'prov.qualifiedGeneration',\n",
       "              'prov.qualifiedUsage',\n",
       "              'prov.used',\n",
       "              'prov.wasAssociatedWith',\n",
       "              'prov.wasInformedBy',\n",
       "              'provone.wasPartOf'},\n",
       "             'prov.Entity': {'prov.wasGeneratedBy',\n",
       "              'provone.dataWasDerivedFrom'},\n",
       "             'prov.Association': {'prov.agent', 'prov.hadPlan'},\n",
       "             'prov.Usage': {'provone.usageHadEntity',\n",
       "              'provone.usageHadInPort'},\n",
       "             'prov.Generation': {'provone.generationHadEntity',\n",
       "              'provone.generationHadOutPort'},\n",
       "             'prov.Collection': {'prov.hadMember'}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_explorer.out_relations_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d4fae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 14:13:56,013 - INFO - Saving results to exploration_results/walks_workflow_2-3_test_run_001\n",
      "Exploring:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-05 14:13:56,015 - INFO - --- Iter 0 ---\n",
      "2025-11-05 14:13:56,016 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,016 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,017 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,017 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,017 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,018 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,018 - INFO - Skipping (generate_graph_query failed) (count=1)\n",
      "2025-11-05 14:13:56,018 - INFO - --- Iter 1 ---\n",
      "2025-11-05 14:13:56,019 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,019 - INFO - No entities found for class provone.Port. Skipping.\n",
      "2025-11-05 14:13:56,020 - INFO - Skipping (generate_graph_query failed) (count=2)\n",
      "2025-11-05 14:13:56,020 - INFO - --- Iter 2 ---\n",
      "2025-11-05 14:13:56,020 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,020 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,020 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,021 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,021 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,021 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,021 - INFO - Skipping (generate_graph_query failed) (count=3)\n",
      "2025-11-05 14:13:56,022 - INFO - --- Iter 3 ---\n",
      "2025-11-05 14:13:56,022 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,022 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,022 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,022 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,023 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,023 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,023 - INFO - Skipping (generate_graph_query failed) (count=4)\n",
      "2025-11-05 14:13:56,023 - INFO - --- Iter 4 ---\n",
      "2025-11-05 14:13:56,024 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,024 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,024 - INFO - Skipping (generate_graph_query failed) (count=5)\n",
      "2025-11-05 14:13:56,024 - INFO - --- Iter 5 ---\n",
      "2025-11-05 14:13:56,025 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,025 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,025 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,026 - INFO - Skipping (generate_graph_query failed) (count=6)\n",
      "2025-11-05 14:13:56,026 - INFO - --- Iter 6 ---\n",
      "2025-11-05 14:13:56,026 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,027 - INFO - Skipping (generate_graph_query failed) (count=7)\n",
      "2025-11-05 14:13:56,027 - INFO - --- Iter 7 ---\n",
      "2025-11-05 14:13:56,027 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,027 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,027 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,028 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,028 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,028 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,028 - INFO - Skipping (generate_graph_query failed) (count=8)\n",
      "2025-11-05 14:13:56,029 - INFO - --- Iter 8 ---\n",
      "2025-11-05 14:13:56,029 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,029 - INFO - Skipping (generate_graph_query failed) (count=9)\n",
      "2025-11-05 14:13:56,029 - INFO - --- Iter 9 ---\n",
      "2025-11-05 14:13:56,030 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,030 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,030 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,030 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,030 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,031 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,031 - INFO - Skipping (generate_graph_query failed) (count=10)\n",
      "2025-11-05 14:13:56,031 - INFO - --- Iter 10 ---\n",
      "2025-11-05 14:13:56,031 - INFO - No entities found for class prov.Entity. Skipping.\n",
      "2025-11-05 14:13:56,031 - INFO - Skipping (generate_graph_query failed) (count=11)\n",
      "2025-11-05 14:13:56,032 - INFO - --- Iter 11 ---\n",
      "2025-11-05 14:13:56,032 - INFO - No entities found for class prov.Entity. Skipping.\n",
      "2025-11-05 14:13:56,032 - INFO - Skipping (generate_graph_query failed) (count=12)\n",
      "2025-11-05 14:13:56,032 - INFO - --- Iter 12 ---\n",
      "2025-11-05 14:13:56,033 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,033 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,033 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,034 - INFO - Already seen. Retrying (4)\n",
      "2025-11-05 14:13:56,034 - INFO - Already seen. Retrying (5)\n",
      "2025-11-05 14:13:56,034 - INFO - Reached maximum attempts in trying to sample new node-edge pairs\n",
      "2025-11-05 14:13:56,034 - INFO - Skipping (generate_graph_query failed) (count=13)\n",
      "2025-11-05 14:13:56,034 - INFO - --- Iter 13 ---\n",
      "2025-11-05 14:13:56,035 - INFO - No entities found for class provone.Port. Skipping.\n",
      "2025-11-05 14:13:56,035 - INFO - Skipping (generate_graph_query failed) (count=14)\n",
      "2025-11-05 14:13:56,035 - INFO - --- Iter 14 ---\n",
      "2025-11-05 14:13:56,035 - INFO - No entities found for class provone.Port. Skipping.\n",
      "2025-11-05 14:13:56,036 - INFO - Skipping (generate_graph_query failed) (count=15)\n",
      "2025-11-05 14:13:56,036 - INFO - --- Iter 15 ---\n",
      "2025-11-05 14:13:56,036 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,036 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,037 - INFO - No entities found for class prov.Generation. Skipping.\n",
      "2025-11-05 14:13:56,037 - INFO - Skipping (generate_graph_query failed) (count=16)\n",
      "2025-11-05 14:13:56,037 - INFO - --- Iter 16 ---\n",
      "2025-11-05 14:13:56,037 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,038 - INFO - No entities found for class provone.Execution. Skipping.\n",
      "2025-11-05 14:13:56,038 - INFO - Skipping (generate_graph_query failed) (count=17)\n",
      "2025-11-05 14:13:56,038 - INFO - --- Iter 17 ---\n",
      "2025-11-05 14:13:56,038 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,039 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,039 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,039 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,039 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,039 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,040 - INFO - Skipping (generate_graph_query failed) (count=18)\n",
      "2025-11-05 14:13:56,040 - INFO - --- Iter 18 ---\n",
      "2025-11-05 14:13:56,040 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,041 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,041 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,041 - INFO - No entities found for class prov.Entity. Skipping.\n",
      "2025-11-05 14:13:56,042 - INFO - Skipping (generate_graph_query failed) (count=19)\n",
      "2025-11-05 14:13:56,042 - INFO - --- Iter 19 ---\n",
      "2025-11-05 14:13:56,042 - INFO - No entities found for class prov.Entity. Skipping.\n",
      "2025-11-05 14:13:56,042 - INFO - Skipping (generate_graph_query failed) (count=20)\n",
      "2025-11-05 14:13:56,043 - INFO - --- Iter 20 ---\n",
      "2025-11-05 14:13:56,043 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,043 - INFO - Skipping (generate_graph_query failed) (count=21)\n",
      "2025-11-05 14:13:56,043 - INFO - --- Iter 21 ---\n",
      "2025-11-05 14:13:56,044 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,044 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,044 - INFO - No entities found for class prov.Usage. Skipping.\n",
      "2025-11-05 14:13:56,044 - INFO - Skipping (generate_graph_query failed) (count=22)\n",
      "2025-11-05 14:13:56,045 - INFO - --- Iter 22 ---\n",
      "2025-11-05 14:13:56,045 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,046 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,046 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,046 - INFO - No entities found for class prov.Collection. Skipping.\n",
      "2025-11-05 14:13:56,046 - INFO - Skipping (generate_graph_query failed) (count=23)\n",
      "2025-11-05 14:13:56,046 - INFO - --- Iter 23 ---\n",
      "2025-11-05 14:13:56,047 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,047 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,047 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,047 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,048 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,048 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,048 - INFO - Skipping (generate_graph_query failed) (count=24)\n",
      "2025-11-05 14:13:56,048 - INFO - --- Iter 24 ---\n",
      "2025-11-05 14:13:56,049 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,049 - INFO - No entities found for class prov.Usage. Skipping.\n",
      "2025-11-05 14:13:56,049 - INFO - Skipping (generate_graph_query failed) (count=25)\n",
      "2025-11-05 14:13:56,050 - INFO - --- Iter 25 ---\n",
      "2025-11-05 14:13:56,050 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,050 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,050 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,050 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,051 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,051 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,051 - INFO - Skipping (generate_graph_query failed) (count=26)\n",
      "2025-11-05 14:13:56,051 - INFO - --- Iter 26 ---\n",
      "2025-11-05 14:13:56,052 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,052 - INFO - Skipping (generate_graph_query failed) (count=27)\n",
      "2025-11-05 14:13:56,052 - INFO - --- Iter 27 ---\n",
      "2025-11-05 14:13:56,052 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,053 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,053 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,053 - INFO - No entities found for class provone.Channel. Skipping.\n",
      "2025-11-05 14:13:56,053 - INFO - Skipping (generate_graph_query failed) (count=28)\n",
      "2025-11-05 14:13:56,053 - INFO - --- Iter 28 ---\n",
      "2025-11-05 14:13:56,054 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,054 - INFO - Skipping (generate_graph_query failed) (count=29)\n",
      "2025-11-05 14:13:56,054 - INFO - --- Iter 29 ---\n",
      "2025-11-05 14:13:56,054 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,054 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,055 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,055 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,055 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,056 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,056 - INFO - Skipping (generate_graph_query failed) (count=30)\n",
      "2025-11-05 14:13:56,056 - INFO - --- Iter 30 ---\n",
      "2025-11-05 14:13:56,056 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,057 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,057 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,057 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,058 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,058 - INFO - Skipping (generate_graph_query failed) (count=31)\n",
      "2025-11-05 14:13:56,058 - INFO - --- Iter 31 ---\n",
      "2025-11-05 14:13:56,059 - INFO - No entities found for class provone.Port. Skipping.\n",
      "2025-11-05 14:13:56,059 - INFO - Skipping (generate_graph_query failed) (count=32)\n",
      "2025-11-05 14:13:56,059 - INFO - --- Iter 32 ---\n",
      "2025-11-05 14:13:56,059 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,059 - INFO - Skipping (generate_graph_query failed) (count=33)\n",
      "2025-11-05 14:13:56,060 - INFO - --- Iter 33 ---\n",
      "2025-11-05 14:13:56,060 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,060 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,060 - INFO - Skipping (generate_graph_query failed) (count=34)\n",
      "2025-11-05 14:13:56,060 - INFO - --- Iter 34 ---\n",
      "2025-11-05 14:13:56,061 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,061 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,062 - INFO - No entities found for class provone.Execution. Skipping.\n",
      "2025-11-05 14:13:56,062 - INFO - Skipping (generate_graph_query failed) (count=35)\n",
      "2025-11-05 14:13:56,062 - INFO - --- Iter 35 ---\n",
      "2025-11-05 14:13:56,062 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,063 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,063 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,063 - INFO - No entities found for class prov.Usage. Skipping.\n",
      "2025-11-05 14:13:56,064 - INFO - Skipping (generate_graph_query failed) (count=36)\n",
      "2025-11-05 14:13:56,064 - INFO - --- Iter 36 ---\n",
      "2025-11-05 14:13:56,064 - INFO - No entities found for class provone.Execution. Skipping.\n",
      "2025-11-05 14:13:56,064 - INFO - Skipping (generate_graph_query failed) (count=37)\n",
      "2025-11-05 14:13:56,065 - INFO - --- Iter 37 ---\n",
      "2025-11-05 14:13:56,065 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,065 - INFO - No entities found for class prov.Usage. Skipping.\n",
      "2025-11-05 14:13:56,065 - INFO - Skipping (generate_graph_query failed) (count=38)\n",
      "2025-11-05 14:13:56,066 - INFO - --- Iter 38 ---\n",
      "2025-11-05 14:13:56,066 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,066 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,066 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,067 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,067 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,067 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,067 - INFO - Skipping (generate_graph_query failed) (count=39)\n",
      "2025-11-05 14:13:56,068 - INFO - --- Iter 39 ---\n",
      "2025-11-05 14:13:56,068 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,068 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,069 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,069 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,069 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,070 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,070 - INFO - Skipping (generate_graph_query failed) (count=40)\n",
      "2025-11-05 14:13:56,070 - INFO - --- Iter 40 ---\n",
      "2025-11-05 14:13:56,070 - INFO - No entities found for class provone.Port. Skipping.\n",
      "2025-11-05 14:13:56,071 - INFO - Skipping (generate_graph_query failed) (count=41)\n",
      "2025-11-05 14:13:56,071 - INFO - --- Iter 41 ---\n",
      "2025-11-05 14:13:56,071 - INFO - No entities found for class provone.Execution. Skipping.\n",
      "2025-11-05 14:13:56,071 - INFO - Skipping (generate_graph_query failed) (count=42)\n",
      "2025-11-05 14:13:56,072 - INFO - --- Iter 42 ---\n",
      "2025-11-05 14:13:56,072 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,072 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,072 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,072 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,073 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,073 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,073 - INFO - Skipping (generate_graph_query failed) (count=43)\n",
      "2025-11-05 14:13:56,074 - INFO - --- Iter 43 ---\n",
      "2025-11-05 14:13:56,074 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,074 - INFO - No entities found for class provone.Program. Skipping.\n",
      "2025-11-05 14:13:56,074 - INFO - Skipping (generate_graph_query failed) (count=44)\n",
      "2025-11-05 14:13:56,075 - INFO - --- Iter 44 ---\n",
      "2025-11-05 14:13:56,075 - INFO - No entities found for class prov.Generation. Skipping.\n",
      "2025-11-05 14:13:56,075 - INFO - Skipping (generate_graph_query failed) (count=45)\n",
      "2025-11-05 14:13:56,076 - INFO - --- Iter 45 ---\n",
      "2025-11-05 14:13:56,076 - INFO - No entities found for class provone.Channel. Skipping.\n",
      "2025-11-05 14:13:56,076 - INFO - Skipping (generate_graph_query failed) (count=46)\n",
      "2025-11-05 14:13:56,076 - INFO - --- Iter 46 ---\n",
      "2025-11-05 14:13:56,077 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,077 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,077 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,078 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,078 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,078 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,078 - INFO - Skipping (generate_graph_query failed) (count=47)\n",
      "2025-11-05 14:13:56,079 - INFO - --- Iter 47 ---\n",
      "2025-11-05 14:13:56,079 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,080 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,100 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,103 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,104 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,105 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,105 - INFO - Skipping (generate_graph_query failed) (count=48)\n",
      "2025-11-05 14:13:56,106 - INFO - --- Iter 48 ---\n",
      "2025-11-05 14:13:56,109 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,109 - INFO - No entities found for class provone.Execution. Skipping.\n",
      "2025-11-05 14:13:56,113 - INFO - Skipping (generate_graph_query failed) (count=49)\n",
      "2025-11-05 14:13:56,115 - INFO - --- Iter 49 ---\n",
      "2025-11-05 14:13:56,117 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,118 - INFO - No entities found for class prov.Generation. Skipping.\n",
      "2025-11-05 14:13:56,118 - INFO - Skipping (generate_graph_query failed) (count=50)\n",
      "2025-11-05 14:13:56,118 - INFO - --- Iter 50 ---\n",
      "2025-11-05 14:13:56,119 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,119 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,119 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,120 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,120 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,120 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,120 - INFO - Skipping (generate_graph_query failed) (count=51)\n",
      "2025-11-05 14:13:56,121 - INFO - --- Iter 51 ---\n",
      "2025-11-05 14:13:56,121 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,121 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,121 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,121 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,122 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,122 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,122 - INFO - Skipping (generate_graph_query failed) (count=52)\n",
      "2025-11-05 14:13:56,122 - INFO - --- Iter 52 ---\n",
      "2025-11-05 14:13:56,122 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,123 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,123 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,123 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,124 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,124 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,124 - INFO - Skipping (generate_graph_query failed) (count=53)\n",
      "2025-11-05 14:13:56,125 - INFO - --- Iter 53 ---\n",
      "2025-11-05 14:13:56,125 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,125 - INFO - No entities found for class prov.Usage. Skipping.\n",
      "2025-11-05 14:13:56,125 - INFO - Skipping (generate_graph_query failed) (count=54)\n",
      "2025-11-05 14:13:56,126 - INFO - --- Iter 54 ---\n",
      "2025-11-05 14:13:56,127 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,129 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,130 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,130 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,131 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,132 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,132 - INFO - Skipping (generate_graph_query failed) (count=55)\n",
      "2025-11-05 14:13:56,133 - INFO - --- Iter 55 ---\n",
      "2025-11-05 14:13:56,133 - INFO - Node 0 has no relations. Retrying (1)\n",
      "2025-11-05 14:13:56,133 - INFO - Node 0 has no relations. Retrying (2)\n",
      "2025-11-05 14:13:56,133 - INFO - Node 0 has no relations. Retrying (3)\n",
      "2025-11-05 14:13:56,134 - INFO - Node 0 has no relations. Retrying (4)\n",
      "2025-11-05 14:13:56,134 - INFO - Node 0 has no relations. Retrying (5)\n",
      "2025-11-05 14:13:56,134 - INFO - Could not sample any edges.\n",
      "2025-11-05 14:13:56,134 - INFO - Skipping (generate_graph_query failed) (count=56)\n",
      "2025-11-05 14:13:56,134 - INFO - --- Iter 56 ---\n",
      "2025-11-05 14:13:56,135 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,135 - INFO - No entities found for class provone.Execution. Skipping.\n",
      "2025-11-05 14:13:56,135 - INFO - Skipping (generate_graph_query failed) (count=57)\n",
      "2025-11-05 14:13:56,135 - INFO - --- Iter 57 ---\n",
      "2025-11-05 14:13:56,135 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,136 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,136 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,136 - INFO - Already seen. Retrying (4)\n",
      "2025-11-05 14:13:56,136 - INFO - Already seen. Retrying (5)\n",
      "2025-11-05 14:13:56,137 - INFO - Reached maximum attempts in trying to sample new node-edge pairs\n",
      "2025-11-05 14:13:56,137 - INFO - Skipping (generate_graph_query failed) (count=58)\n",
      "2025-11-05 14:13:56,137 - INFO - --- Iter 58 ---\n",
      "2025-11-05 14:13:56,137 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,137 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,138 - INFO - Already seen. Retrying (3)\n",
      "2025-11-05 14:13:56,138 - INFO - Already seen. Retrying (4)\n",
      "2025-11-05 14:13:56,138 - INFO - Already seen. Retrying (5)\n",
      "2025-11-05 14:13:56,138 - INFO - Reached maximum attempts in trying to sample new node-edge pairs\n",
      "2025-11-05 14:13:56,138 - INFO - Skipping (generate_graph_query failed) (count=59)\n",
      "2025-11-05 14:13:56,139 - INFO - --- Iter 59 ---\n",
      "2025-11-05 14:13:56,139 - INFO - Already seen. Retrying (1)\n",
      "2025-11-05 14:13:56,139 - INFO - Already seen. Retrying (2)\n",
      "2025-11-05 14:13:56,139 - INFO - No entities found for class provone.Port. Skipping.\n",
      "2025-11-05 14:13:56,139 - INFO - Skipping (generate_graph_query failed) (count=60)\n",
      "Exploring:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "2025-11-05 14:13:56,140 - INFO - Stopping exploration: reached maximum attempts (60)\n",
      "2025-11-05 14:13:56,141 - INFO - Saved walks to exploration_results/walks_workflow_2-3_test_run_001/results.json\n",
      "2025-11-05 14:13:56,142 - INFO - Saved exploration statistics to exploration_results/walks_workflow_2-3_test_run_001/stats.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_explorer.explore(\n",
    "    n_walks=10,\n",
    "    edge_lengths=[2, 3],\n",
    "    max_retries_per_iter=5,\n",
    "    always_ground_classes=False,\n",
    "    always_ground_literals=True,\n",
    "    sexpr_type_constraint=True,\n",
    "    n_per_pattern=1,\n",
    "    use_functions=None,\n",
    "    max_skip=50,\n",
    "    verbose=True,\n",
    "    max_retries=100,\n",
    "    out_dir=\"exploration_results\",\n",
    "    run_id=\"test_run_001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
