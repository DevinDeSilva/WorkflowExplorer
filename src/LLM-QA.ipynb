{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef25fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2025 21:19:44 - INFO - src.explorer_updates -   Loading .env file from: /home/desild/work/research/chatbs/v2/.env\n",
      "11/15/2025 21:19:44 - INFO - src.explorer_updates -   Loading config: /home/desild/work/research/chatbs/v2/prov.config.yaml\n",
      "11/15/2025 21:19:44 - INFO - src.explorer_updates -   Loading metadata: /home/desild/work/research/chatbs/v2/data/workflow/50_sample_graph/chatbs_sample_metadata.json\n",
      "11/15/2025 21:19:44 - INFO - src.explorer_updates -   Initializing GraphManager...\n",
      "11/15/2025 21:19:44 - INFO - src.explorer_updates -   Graph loaded with 24073 triples.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from functools import partial\n",
    "from rdflib import Graph, Literal, URIRef\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import dycomutils as common_utils\n",
    "from typing import List, Dict, Any, Optional, Set, Tuple, DefaultDict\n",
    "import openai\n",
    "\n",
    "sys.path.append(\"/home/desild/work/research/chatbs/v2\")\n",
    "\n",
    "from src.utils.helpers import setup_logger\n",
    "from src.utils.parser import graph_query_to_sexpr, is_inv_rel, get_inv_rel, graph_query_to_sparql\n",
    "from src.utils.kg import get_readable_relation, get_readable_class, get_non_literals, get_nodes_by_class, \\\n",
    "    get_reverse_relation, get_reverse_readable_relation, prune_graph_query, legal_class, legal_relation\n",
    "from src.utils.arguments import Arguments\n",
    "from src.utils.sparql import SPARQLUtil, get_freebase_label, get_freebase_literals_by_cls_rel, \\\n",
    "    get_freebase_entid_lbl_by_cls\n",
    "from src.utils.maps import literal_map\n",
    "\n",
    "from transformers import set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s', datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from src.explorer_updates import Explorer, ExecutableProgram\n",
    "from src.utils.graph_manager import GraphManager, regex_add_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ca5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_chat(system_prompt: str, user_prompt: str, model_version: str, structured_output: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Sends a chat request to an OpenAI-compatible API.\n",
    "    R: llm_chat\n",
    "    \"\"\"\n",
    "    client = None\n",
    "    # R: if ((startsWith(model_version, \"gpt-\")) || (startsWith(model_version, \"o1-\")))\n",
    "    if model_version.startswith(\"gpt-\") or model_version.startswith(\"o1-\"):\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not set in .env file\")\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        # R: base_url = \"http://idea-llm-01.idea.rpi.edu:5000/v1/\"\n",
    "        client = openai.OpenAI(\n",
    "            base_url=\"http://idea-llm-01.idea.rpi.edu:5000/v1/\",\n",
    "            api_key=os.getenv(\"LOCAL_LLM_API_KEY\", \"no-key-needed\") # Add your local key to .env if needed\n",
    "        )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    chat_params = {\n",
    "        \"model\": model_version,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    # R: if (!is.null(structured_output))\n",
    "    if structured_output:\n",
    "        log.info(\"Requesting structured (JSON) output from LLM.\")\n",
    "        # This is the modern way to request JSON from OpenAI\n",
    "        chat_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        \n",
    "    try:\n",
    "        response = client.chat.completions.create(**chat_params)\n",
    "        answer = response.choices[0].message.content\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error in LLM chat: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def update_answer(system_prompt: str, user_prompt: str, generated_answer: str, error_message: str, model_version: str) -> str:\n",
    "    \"\"\"\n",
    "    Asks the LLM to correct a previous, failed response.\n",
    "    R: update_answer\n",
    "    \"\"\"\n",
    "    recorrection_template = f\"\"\"\n",
    "    User prompt : {user_prompt}\n",
    "    Incorrect generated answer : {generated_answer}\n",
    "    Error message : {error_message}\n",
    "    Analyze the original user prompt, the incorrect answer, and the error message. Identify where the generated response failed to meet the promptâ€™s intent. Then, provide a revised answer.\n",
    "    \"\"\"\n",
    "    return llm_chat(system_prompt, recorrection_template, model_version)\n",
    "\n",
    "\n",
    "def create_timestamp_id(prefix:str):\n",
    "    \"\"\"\n",
    "    Creates a unique identifier based on the current timestamp.\n",
    "    R: create_timestamp_id\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    return f\"{prefix}_{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17470668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup & Configuration ---\n",
    "ROOT_DIR = os.path.abspath(\"/home/desild/work/research/chatbs\")\n",
    "V2_DIR = os.path.join(ROOT_DIR, \"v2\")\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4313477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2025 21:19:44 - INFO - __main__ -   Loading .env file from: /home/desild/work/research/chatbs/v2/.env\n",
      "11/15/2025 21:19:44 - INFO - __main__ -   Loading config: /home/desild/work/research/chatbs/v2/prov.config.yaml\n",
      "11/15/2025 21:19:44 - INFO - __main__ -   Loading metadata: /home/desild/work/research/chatbs/v2/data/workflow/chatbs_sample_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Load .env file from the specified path\n",
    "# R: load_dot_env(\"../ChatBS-NexGen/.env\")\n",
    "env_path = os.path.join(V2_DIR, \".env\")\n",
    "log.info(f\"Loading .env file from: {env_path}\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Load YAML config\n",
    "# R: config <- yaml::read_yaml(...)\n",
    "config_path = os.path.join(V2_DIR, \"prov.config.yaml\")\n",
    "log.info(f\"Loading config: {config_path}\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load JSON metadata\n",
    "# R: ttl_metadata <- readLines(\"QGraph_metadata.json\")\n",
    "metadata_path = os.path.join(V2_DIR, \"data/workflow/chatbs_sample_metadata.json\")\n",
    "log.info(f\"Loading metadata: {metadata_path}\")\n",
    "with open(metadata_path, 'r') as f:\n",
    "    ttl_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f7b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2025 21:19:44 - INFO - src.utils.graph_manager -   Initializing GraphManager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2025 21:19:45 - INFO - src.utils.graph_manager -   Graph loaded with 27756 triples.\n"
     ]
    }
   ],
   "source": [
    "graph_manager = GraphManager(config, os.path.join(V2_DIR, \"data/workflow/explored_programs_fno.ttl\"))\n",
    "schema = common_utils.serialization.load_json(os.path.join(V2_DIR, \"data/workflow/schema.json\"))\n",
    "definitions = {'class_definitions':schema['classes'], 'relation_definitions':{k:v[\"description\"] for k,v in schema['relations'].items()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c997a026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n?exploration_id dc:title ?question .\\n    OPTIONAL {\\n        ?exploration_id prov:wasAssociatedWith ?program .\\n        ?program fnoc:hasFunctionMapping ?mapping .\\n        ?mapping fnom:mapsToFunction ?function .\\n        ?function fnom:hasInputParameter ?input_param .\\n        ?input_param fnom:parameterType ?cls .\\n        ?cls rdfs:subClassOf* dbo:Category .\\n        ?cls rdfs:label ?category_label .\\n    }\\n    BIND(GROUP_CONCAT(DISTINCT ?category_label; SEPARATOR=\"->\") AS ?categories)\\n}GROUP BY ?exploration_id ?question ?categories \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "?exploration_id dc:title ?question .\n",
    "    OPTIONAL {\n",
    "        ?exploration_id prov:wasAssociatedWith ?program .\n",
    "        ?program fnoc:hasFunctionMapping ?mapping .\n",
    "        ?mapping fnom:mapsToFunction ?function .\n",
    "        ?function fnom:hasInputParameter ?input_param .\n",
    "        ?input_param fnom:parameterType ?cls .\n",
    "        ?cls rdfs:subClassOf* dbo:Category .\n",
    "        ?cls rdfs:label ?category_label .\n",
    "    }\n",
    "    BIND(GROUP_CONCAT(DISTINCT ?category_label; SEPARATOR=\"->\") AS ?categories)\n",
    "}GROUP BY ?exploration_id ?question ?categories \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da48c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_ALL_QUESTIONS = \"\"\"\n",
    "\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX ep: <http://linkedu.eu/dedalo/explanationPattern.owl#>\n",
    "PREFIX eo: <https://purl.org/heals/eo#>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX food: <http://purl.org/heals/food/>\n",
    "PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "PREFIX provone: <http://purl.org/provone#>\n",
    "PREFIX sio:<http://semanticscience.org/resource/>\n",
    "PREFIX cwfo: <http://cwf.tw.rpi.edu/vocab#>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms#>\n",
    "PREFIX user: <http://testwebsite/testUser#>\n",
    "PREFIX DFColumn: <http://testwebsite/testDFColumn#>\n",
    "PREFIX fnom: <https://w3id.org/function/vocabulary/mapping#>\n",
    "PREFIX fnoi: <hhttps://w3id.org/function/vocabulary/implementation#>\n",
    "PREFIX fnoc: <https://w3id.org/function/vocabulary/composition/0.1.0/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "PREFIX dbp: <http://dbpedia.org/property/>\n",
    "PREFIX dbt: <http://dbpedia.org/resource/Template:>\n",
    "PREFIX ques: <http://atomic_questions.org/>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "PREFIX fno: <https://w3id.org/function/vocabulary/core#>\n",
    "    \n",
    "SELECT ?mapping ?question_lbl ?paths\n",
    "WHERE {\n",
    "    ?mapping a fno:Mapping .\n",
    "    ?mapping fno:function ?function .\n",
    "    ?function fno:solves ?question .\n",
    "    ?function fno:name ?paths .\n",
    "    ?question fno:name ?question_lbl .\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ques_info = graph_manager.query(GET_ALL_QUESTIONS)\n",
    "ques_info['entity'] = ques_info['paths'].apply(lambda x: [y.strip() for i,y in enumerate(x.split(\"->\")) if i%2==0])\n",
    "ques_info['relations'] = ques_info['paths'].apply(lambda x: [y.strip() for i,y in enumerate(x.split(\"->\")) if i%2==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc02e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DF_verberlize(df: pd.DataFrame, cols: List[str], sep: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame column to a formatted string.\n",
    "    R: build_DF_to_string\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for _, row in df.iterrows():\n",
    "        col_lines = []\n",
    "        for col in cols:\n",
    "            col_lines.append(f\"{row[col]}\")\n",
    "        lines.append(f\"{sep.join(col_lines)}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a630c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provone:Program - A computational task that consumes and produces data: Can be atomic or composite:\n",
      "provone:Port - Enables a Program to send or receive Entity items (Data, Visualization, or Document instances):\n",
      "provone:Channel - Provides a connection between Ports that are defined for Programs:\n",
      "provone:Execution - Represents the execution of a Program: If the Program is a Workflow, this is the trace:\n",
      "provone:User - The person(s) responsible for the execution of an Execution: Subclass of prov:Agent:\n",
      "provone:Data - Represents the basic unit of information consumed or produced by a Program: Subclass of provone:Entity\n",
      "prov:Collection - An entity that provides a structure to some constituents, which are themselves entities:\n",
      "prov:Association - An assignment of responsibility to an agent (User) for an activity (Execution), specifying a plan (Program):\n",
      "prov:Usage - The beginning of utilizing an entity by an activity (Execution): Qualifies the 'used' relation:\n",
      "prov:Generation - The completion of production of a new entity by an activity (Execution): Qualifies the 'wasGeneratedBy' relation:\n",
      "eo:AITask - An AI task performed within a workflow:\n",
      "eo:ObjectRecord - A data record of in the workflow:\n"
     ]
    }
   ],
   "source": [
    "class_def = pd.DataFrame(definitions['class_definitions']).T.reset_index()\n",
    "relation_def = pd.DataFrame({k:{\"description\":v} for k,v in definitions['relation_definitions'].items()}).T.reset_index()\n",
    "\n",
    "class_def.columns = ['class', 'description']\n",
    "relation_def.columns = ['relation', 'description']\n",
    "\n",
    "print(build_DF_verberlize(class_def, ['class', 'description'], \" - \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d98c5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_markdown(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts JSON from a markdown code block.\n",
    "    R: extract_json_from_markdown_stringr (simulated)\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def return_json_formatted(model_response: str):\n",
    "    \"\"\"\n",
    "    Parses a JSON string, with retries for markdown blocks.\n",
    "    R: return_json_formatted\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # R: tryCatch({ fromJSON(model_response) })\n",
    "        return json.loads(model_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        log.warning(f\"Error parsing JSON (layer 1): {e}. Trying to extract from markdown.\")\n",
    "        try:\n",
    "            # R: tryCatch({ ... extract_json ... })\n",
    "            json_content = extract_json_from_markdown(model_response)\n",
    "            if json_content:\n",
    "                return json.loads(json_content)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON markdown content extracted.\")\n",
    "        except Exception as e2:\n",
    "            # R: ... return(data.frame(question = NA, explanation = NA))\n",
    "            log.error(f\"Error in parsing JSON (layer 2): {e2}\")\n",
    "            # Return a list as the prompt expects, even on failure\n",
    "            return []\n",
    "\n",
    "def decide_probable_entity(question : str, class_schema: Dict[str, str], relation_schema: Dict[str, str]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Uses an LLM to decide the most probable entity from a list of candidates based on the prompt.\n",
    "    R: decide_probable_entity\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    The user's question is: {question}  \n",
    "    Please generate an answer that conforms to the above format:\n",
    "    \"\"\"\n",
    "\n",
    "    c_schema = \"\\n\".join([f\"{k} - {v}\" for k, v in class_schema.items()])\n",
    "    r_schema = \"\\n\".join([f\"{k} - {v}\" for k, v in relation_schema.items()])\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    I am developing a knowledge graph enhanced question answering system.  \n",
    "    Your task is to extract conditional entities and their types and destination entities and their types from user input questions.  \n",
    "    Please select the type of entity from the following table:  Each line describes an entity type, in the format of -- entity type (description information) \n",
    "    \n",
    "    #class\n",
    "    {class_schema}  \n",
    "    \n",
    "    #relation\n",
    "    {relation_schema}\n",
    "    \n",
    "    Rules:  \n",
    "    \n",
    "    -The conditional entity is the known information provided in the problem; \n",
    "    -The target entity is the content that the user wants to query in the problem; \n",
    "    - \n",
    "    \n",
    "    Output format: Give me a JSON list of candidate entities.\n",
    "    \n",
    "    Example: \n",
    "    \n",
    "    Example1: \n",
    "    Input: \"what are the ingredients that were suggested by the LLM during the experiment ?\" \n",
    "    Output: {\n",
    "        \"conditional_entities\": [\n",
    "            {\"name\": \"ingredients\", \"type\": [\"provone:Data\", \"provone:Collection\"]},\n",
    "            {\"name\": \"LLM during the experiment\", \"type\": [\"provone:Execution\", \"provone:Program\"]}\n",
    "        ],\n",
    "        \"destination_entities\": Null\n",
    "    }  \n",
    "    \n",
    "    Example2: \n",
    "    Input: \"what are the sparql_query_extractor program executions that returned Null?\" \n",
    "    Output: {\n",
    "        \"conditional_entities\": [\n",
    "            {\"name\": \"sparql_query_extractor\", \"type\": [\"provone:Program\"]},\n",
    "            {\"name\": \"Null\", \"type\": [\"provone:Data\"]}\n",
    "        ],\n",
    "        \"destination_entities\": [\n",
    "            {\"name\": \"executions\", \"type\": [\"provone:Execution\"]}\n",
    "        ]\n",
    "    }  \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = regex_add_strings(\n",
    "        system_prompt,\n",
    "        class_schema = c_schema, \n",
    "        relation_schema = r_schema\n",
    "    )\n",
    "    \n",
    "    #print(system_prompt)\n",
    "\n",
    "    # system_prompt = f\"\"\"\n",
    "    # You are an expert at ontology designing. use the following schema to identify relevant classes.\n",
    "    # Schema: {json.dumps(schema, indent=2)}\n",
    "    \n",
    "    # Please select the most probable entity that matches the prompt. Respond with the number corresponding to your choice.\n",
    "    # and return the output as a list of candidate entities in JSON format.\n",
    "    # \"\"\"\n",
    "    \n",
    "    response = llm_chat(system_prompt, user_prompt, 'gpt-4o')\n",
    "    return return_json_formatted(response)\n",
    "\n",
    "def string_closest_match(target: str, candidates: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Finds the closest matching string from a list of candidates.\n",
    "    R: string_closest_match\n",
    "    \"\"\"\n",
    "    target_set = set(target.split(\" \"))\n",
    "    candidates_set = [set(c.split(\"|\")[0].split(\" \")) for c in candidates]\n",
    "    matches = [len(c.intersection(target_set))/len(c.union(target_set)) for c in candidates_set]\n",
    "    _maxv = np.max(matches)\n",
    "    _argmax = np.argmax(matches)\n",
    "    \n",
    "    # print(target_set)\n",
    "    # print(candidates_set)\n",
    "    # print(\"Matches:\", matches)\n",
    "    # print(candidates[_argmax])\n",
    "    return candidates[_argmax]\n",
    "\n",
    "class QuestionBreakdownAgent:\n",
    "    \"\"\"\n",
    "    Breaks down a question into important classes and relevant sub-questions.\n",
    "    R: QuestionBreakdownAgent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, schema: Dict[str, str], definitions: Dict[str, Any], ques_info: pd.DataFrame):\n",
    "        self.schema = schema\n",
    "        self.definitions = definitions\n",
    "        \n",
    "        self.USER_PROMPT_POE = \"\"\"\n",
    "        Given the complex question: \"{question}\", break it down into a series of sub-questions using the provided atomic questions.\n",
    "        Each sub-question should be linked to an atomic question from the provided list.\n",
    "        Formulate a plan of execution of these questions to at the end achieve the answer to the complex question.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.SYSTEM_PROMPT_POE = \"\"\"\n",
    "        You are an expert question breakdown agent. Given a complex question, you will break it down guided by provided atomic questions\n",
    "        Formulate a plan of execution of these questions to at the end achieve the answer to the complex question. External information is \n",
    "        obtained by querying a knowledge graph using those atomic questions to which SPARQL queries are available.\n",
    "        \n",
    "        You may need to do operations such as filtering, sorting, counting, aggregating, etc., based on the nature of the question. if so provide `Null` \n",
    "        as the atomic question used for that step. \n",
    "        When possible give multiple possible ways to calculate.\n",
    "\n",
    "        IN THE FIRST STEP, YOU MUST ALWAYS IDENTIFY A main class to retrive objects from. next steps would be the traversal of the knowledge graph. \n",
    "        ensure the path is reachable in the knowledge graph. \n",
    "\n",
    "        The available atomic questions are provided below in the format of question | path traversed in the knowledge graph:\n",
    "        {atomic_questions}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.EXPL1_POE = \"\"\"\n",
    "        QUESTION: \"how many programs are in this system?\"\n",
    "        PLANS:\n",
    "        ### Plan 1:\n",
    "        ### Step 1:\n",
    "        - Sub question: Identify the program in the whole system.\n",
    "        - Atomic Question Used: \"Explores objects of a given class in the RDF graph\"\n",
    "        \n",
    "        #### Step 2: Count the entities\n",
    "        - Sub-question: Count the number of programs identified in Step 1.\n",
    "        - Atomic Question Used: Null\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.EXPL2_POE = \"\"\"\n",
    "        QUESTION: \"what are the programs that have AI capabilities?\"\n",
    "        PLANS:\n",
    "        \n",
    "        ### Plan 1:\n",
    "        ### Step 1:\n",
    "        - Sub question: Identify the program in the whole system.\n",
    "        - Atomic Question Used: \"Explores objects of a given class in the RDF graph\"\n",
    "        \n",
    "        #### Step 2: \n",
    "        - Sub-question: What AI task is the output of this program?.\n",
    "        - Atomic Question Used: \"Explores objects of a given class in the RDF graph\"\n",
    "\n",
    "        #### Step 3: \n",
    "        - Sub-question: Filter the programs that have AI capabilities.\n",
    "        - Atomic Question Used: Null\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.USER_PROMPT_FORMAT = \"\"\"\n",
    "        Convert the text to a structured format in JSON as shown in the example.\n",
    "        \n",
    "        # Text\n",
    "        {text}\n",
    "        \n",
    "        JSON:\n",
    "        \"\"\"\n",
    "        \n",
    "        self.SYSTEM_PROMPT_FORMAT = \"\"\"\n",
    "        You are a data formatter. Given a text in structured format, you will convert it into JSON.\n",
    "        The output should be a list of plans, where each plan contains steps with sub-questions and used atomic questions.\n",
    "        Use the example below to guide your formatting:\n",
    "        \n",
    "        # Text\n",
    "        PLANS:\n",
    "        ### Plan 1:\n",
    "        #### Step 1: Identify the main class\n",
    "        - Sub question: Identify the program in the whole system.\n",
    "        - Atomic Question Used: \"Explores objects of a given class in the RDF graph\"\n",
    "        \n",
    "        #### Step 2: Count the entities\n",
    "        - Sub-question: Count the number of programs identified in Step 1.\n",
    "        - Atomic Question Used: Null\n",
    "\n",
    "        JSON:\n",
    "        [{\n",
    "            \"step1\": {\n",
    "                \"sub-question\": \"Identify the program in the whole system\",\n",
    "                \"used_atomic_question\": \"Explores objects of a given class in the RDF graph\"\n",
    "            },\n",
    "            \"step2\": {\n",
    "                \"sub-question\": \"Count the number of programs identified in Step 1\",\n",
    "                \"used_atomic_question\": \"Null\"\n",
    "            }\n",
    "         }]\n",
    "        \"\"\"\n",
    "        self.logs = {}\n",
    "        self.setup_system_prompt(ques_info)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def setup_system_prompt(self, question_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Sets up the system prompt with atomic questions.\n",
    "        R: setup_system_prompt\n",
    "        \"\"\"\n",
    "        atomic_questions_str = build_DF_verberlize(\n",
    "            question_df,\n",
    "            ['question_lbl', 'paths'],\n",
    "            \" | \"\n",
    "        )\n",
    "        \n",
    "        self.SYSTEM_PROMPT_POE = regex_add_strings(\n",
    "            self.SYSTEM_PROMPT_POE,\n",
    "            atomic_questions = atomic_questions_str\n",
    "        )\n",
    "        \n",
    "        self.full_system_prompt_poe = \"\\n\\n\".join([\n",
    "            self.SYSTEM_PROMPT_POE,\n",
    "            self.EXPL1_POE,\n",
    "            self.EXPL2_POE\n",
    "        ])\n",
    "    \n",
    "    def sub_questions(self, question: str) -> Any:\n",
    "        \"\"\"\n",
    "        Breaks down the question using important classes and relevant sub-questions.\n",
    "        R: break_down_question\n",
    "        \"\"\"\n",
    "        self.logs[\"sub_question\"] = {}\n",
    "        \n",
    "        user_prompt = regex_add_strings(\n",
    "            self.USER_PROMPT_POE,\n",
    "            question = question\n",
    "        )\n",
    "        \n",
    "        response = llm_chat(self.full_system_prompt_poe, user_prompt, 'gpt-4o')\n",
    "        self.logs[\"sub_question\"][\"sub_questions_response\"] = {\n",
    "            \"system_prompt\": self.full_system_prompt_poe, \n",
    "            \"user_prompt\": user_prompt, \n",
    "            \"response\": response\n",
    "            }\n",
    "        \n",
    "        user_prompt = regex_add_strings(\n",
    "            self.USER_PROMPT_FORMAT,\n",
    "            text = response\n",
    "        )\n",
    "        \n",
    "        response = llm_chat(self.SYSTEM_PROMPT_FORMAT, user_prompt, 'gpt-4o')\n",
    "        self.logs[\"sub_question\"][\"format_response\"] = {\n",
    "            \"system_prompt\": self.SYSTEM_PROMPT_FORMAT,\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"response\": response\n",
    "        }\n",
    "        \n",
    "        response = return_json_formatted(response)\n",
    "        self.logs[\"sub_question\"][\"final_response\"] = response\n",
    "        return response\n",
    "    \n",
    "    def plan_of_execution(self, question: str) -> Any:\n",
    "        \"\"\"\n",
    "        Generates a plan of execution for the question.\n",
    "        R: plan_of_execution\n",
    "        \"\"\"\n",
    "        \n",
    "        plans = self.sub_questions(question)\n",
    "        best_plan = None\n",
    "        best_cost = float('inf')\n",
    "        for plan in plans:\n",
    "            plan_cost = 0\n",
    "            for i, step in enumerate(plan.values()):\n",
    "                atomic_question = step.get(\"used_atomic_question\", \"\")\n",
    "                matched_question = string_closest_match(atomic_question, ques_info['question_lbl'].tolist())\n",
    "                if matched_question:\n",
    "                    row = ques_info[ques_info['question_lbl'] == matched_question].iloc[0]\n",
    "                    path_len = len(row['entity']) + len(row['relations'])\n",
    "                    plan_cost += pow(2.71828, path_len)\n",
    "                   \n",
    "                else:\n",
    "                    raise ValueError(\"No matching atomic question found.\")\n",
    "                \n",
    "            if plan_cost < best_cost:\n",
    "                best_cost = plan_cost\n",
    "                best_plan = plan\n",
    "        \n",
    "        self.logs[\"best_plan\"] = best_plan\n",
    "        self.logs[\"best_plan_cost\"] = best_cost\n",
    "        return best_plan\n",
    "    \n",
    "    \n",
    "    def save_logs(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Saves the logs to a JSON file.\n",
    "        R: save_logs\n",
    "        \"\"\"\n",
    "        _dir, _ = os.path.split(filepath)\n",
    "        if not os.path.exists(_dir):\n",
    "            os.makedirs(_dir)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.logs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c61e24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapping</th>\n",
       "      <th>question_lbl</th>\n",
       "      <th>paths</th>\n",
       "      <th>entity</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://atomic_questions.org/eprog0_mapping_202...</td>\n",
       "      <td>Who was the user associated with this execution?</td>\n",
       "      <td>provone:Execution-&gt;prov:wasAssociatedWith-&gt;pro...</td>\n",
       "      <td>[provone:Execution, provone:User]</td>\n",
       "      <td>[prov:wasAssociatedWith]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://atomic_questions.org/eprog0_mapping_202...</td>\n",
       "      <td>Explores objects of a given class in the RDF g...</td>\n",
       "      <td>provone:Execution -&gt; provone:Port -&gt; provone:D...</td>\n",
       "      <td>[provone:Execution, provone:Data, provone:Prog...</td>\n",
       "      <td>[provone:Port, provone:Channel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://atomic_questions.org/eprog10_mapping_20...</td>\n",
       "      <td>What data entities were generated by this exec...</td>\n",
       "      <td>provone:Execution-&gt;prov:qualifiedGeneration-&gt;p...</td>\n",
       "      <td>[provone:Execution, prov:Generation, provone:D...</td>\n",
       "      <td>[prov:qualifiedGeneration, provone:hadEntity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://atomic_questions.org/eprog10_mapping_20...</td>\n",
       "      <td>For a given prop value of class provone:Data, ...</td>\n",
       "      <td>provone:Data -&gt; dbp:sugars</td>\n",
       "      <td>[provone:Data]</td>\n",
       "      <td>[dbp:sugars]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://atomic_questions.org/eprog11_mapping_20...</td>\n",
       "      <td>Which channels are connected to the input port...</td>\n",
       "      <td>provone:Execution-&gt;prov:qualifiedUsage-&gt;prov:U...</td>\n",
       "      <td>[provone:Execution, prov:Usage, provone:Port, ...</td>\n",
       "      <td>[prov:qualifiedUsage, provone:hadInPort, provo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mapping  \\\n",
       "0  http://atomic_questions.org/eprog0_mapping_202...   \n",
       "1  http://atomic_questions.org/eprog0_mapping_202...   \n",
       "2  http://atomic_questions.org/eprog10_mapping_20...   \n",
       "3  http://atomic_questions.org/eprog10_mapping_20...   \n",
       "4  http://atomic_questions.org/eprog11_mapping_20...   \n",
       "\n",
       "                                        question_lbl  \\\n",
       "0   Who was the user associated with this execution?   \n",
       "1  Explores objects of a given class in the RDF g...   \n",
       "2  What data entities were generated by this exec...   \n",
       "3  For a given prop value of class provone:Data, ...   \n",
       "4  Which channels are connected to the input port...   \n",
       "\n",
       "                                               paths  \\\n",
       "0  provone:Execution->prov:wasAssociatedWith->pro...   \n",
       "1  provone:Execution -> provone:Port -> provone:D...   \n",
       "2  provone:Execution->prov:qualifiedGeneration->p...   \n",
       "3                         provone:Data -> dbp:sugars   \n",
       "4  provone:Execution->prov:qualifiedUsage->prov:U...   \n",
       "\n",
       "                                              entity  \\\n",
       "0                  [provone:Execution, provone:User]   \n",
       "1  [provone:Execution, provone:Data, provone:Prog...   \n",
       "2  [provone:Execution, prov:Generation, provone:D...   \n",
       "3                                     [provone:Data]   \n",
       "4  [provone:Execution, prov:Usage, provone:Port, ...   \n",
       "\n",
       "                                           relations  \n",
       "0                           [prov:wasAssociatedWith]  \n",
       "1                    [provone:Port, provone:Channel]  \n",
       "2      [prov:qualifiedGeneration, provone:hadEntity]  \n",
       "3                                       [dbp:sugars]  \n",
       "4  [prov:qualifiedUsage, provone:hadInPort, provo...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe3d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_down_question(question: str, schema: Dict, uniq_id: str) -> Any:\n",
    "    \"\"\"\n",
    "    Breaks down a question into its components and adds them to the graph.\n",
    "    R: break_down_question\n",
    "    \"\"\"\n",
    "    \n",
    "    important_classes = decide_probable_entity(\n",
    "            question, \n",
    "            schema['class_definitions'], \n",
    "            schema['relation_definitions']\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not important_classes:\n",
    "        log.error(\"No important classes identified.\")\n",
    "        relevant_questions = ques_info\n",
    "    else:\n",
    "        if important_classes['conditional_entities']:\n",
    "            relevant_conditional_types = [x[\"type\"] for x in important_classes['conditional_entities']]\n",
    "        else:\n",
    "            relevant_conditional_types = []\n",
    "            \n",
    "        if important_classes['destination_entities']:\n",
    "            relevant_destination_types = [x[\"type\"] for x in important_classes['destination_entities']]\n",
    "        else:\n",
    "            relevant_destination_types = []\n",
    "        \n",
    "        relevant_types = relevant_conditional_types + relevant_destination_types\n",
    "        relevant_types = list(set(item for sublist in relevant_types for item in sublist))\n",
    "        \n",
    "        print(relevant_types)\n",
    "        if not relevant_types:\n",
    "            log.error(\"No relevant types identified from important classes.\")\n",
    "            relevant_questions = ques_info\n",
    "        else:\n",
    "            relevant_questions = ques_info.loc[\n",
    "                ques_info['entity'].apply(\n",
    "                    lambda x: len(set(x).intersection(set(relevant_types))) > 0\n",
    "                    )\n",
    "                ]\n",
    "    relevant_questions = relevant_questions.reset_index(drop=True)\n",
    "    composer = QuestionBreakdownAgent(schema, definitions, relevant_questions)\n",
    "    plans = composer.plan_of_execution(question)\n",
    "    \n",
    "    composer.save_logs(f\"./logs/question_breakdown_{uniq_id}.json\")\n",
    "    return plans\n",
    "\n",
    "def answer_question(question: str):\n",
    "    \"\"\"\n",
    "    Answers a question using the important classes and the graph.\n",
    "    R: answer_question\n",
    "    \"\"\"\n",
    "    plan = break_down_question(\n",
    "        question, definitions, create_timestamp_id(\"user_question\")\n",
    "    )\n",
    "    \n",
    "    return {\"question\":question, \"plan\":plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b68b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dome = answer_question(\n",
    "#     \"what are the programs generated by AI?\", \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db7976f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sparql(llm_output: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts a SPARQL query from a markdown code block.\n",
    "\n",
    "    Args:\n",
    "        llm_output: The text output from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        The extracted SPARQL query as a string, or None if not found.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find content inside ```sparql ... ```\n",
    "    # re.DOTALL makes '.' match newline characters\n",
    "    # re.IGNORECASE makes 'sparql' case-insensitive\n",
    "    pattern = r\"```sparql\\s*([\\s\\S]+?)\\s*```\"\n",
    "    \n",
    "    match = re.search(pattern, llm_output, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # Return the first capturing group (the content)\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "class SPARQLAgent:\n",
    "    \"\"\"\n",
    "    Breaks down a question into important classes and relevant sub-questions.\n",
    "    R: QuestionBreakdownAgent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, plan: Dict[str, Dict[str, str]], definitions: Dict[str, Any]):\n",
    "        self.plan = plan[\"plan\"]\n",
    "        self.main_question = plan[\"question\"]\n",
    "        self.definitions = definitions\n",
    "        self.logs=  {}\n",
    "    \n",
    "    def get_function_sparql(self, map_id: str):\n",
    "        SPARQL_query = \"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "        PREFIX ep: <http://linkedu.eu/dedalo/explanationPattern.owl#>\n",
    "        PREFIX eo: <https://purl.org/heals/eo#>\n",
    "        PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "        PREFIX food: <http://purl.org/heals/food/>\n",
    "        PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "        PREFIX provone: <http://purl.org/provone#>\n",
    "        PREFIX sio:<http://semanticscience.org/resource/>\n",
    "        PREFIX cwfo: <http://cwf.tw.rpi.edu/vocab#>\n",
    "        PREFIX dcterms: <http://purl.org/dc/terms#>\n",
    "        PREFIX user: <http://testwebsite/testUser#>\n",
    "        PREFIX DFColumn: <http://testwebsite/testDFColumn#>\n",
    "        PREFIX fnom: <https://w3id.org/function/vocabulary/mapping#>\n",
    "        PREFIX fnoi: <hhttps://w3id.org/function/vocabulary/implementation#>\n",
    "        PREFIX fnoc: <https://w3id.org/function/vocabulary/composition/0.1.0/>\n",
    "        PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        PREFIX dbp: <http://dbpedia.org/property/>\n",
    "        PREFIX dbt: <http://dbpedia.org/resource/Template:>\n",
    "        PREFIX ques: <http://atomic_questions.org/>\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        PREFIX fno: <https://w3id.org/function/vocabulary/core#>\n",
    "\n",
    "        SELECT ?imp ?function ?param_map ?param_desc ?return_map ?return_desc\n",
    "        WHERE {\n",
    "            <{map_id}> fno:implementation/rdfs:label ?imp ;\n",
    "                       fno:function ?function ;\n",
    "                       fno:parameterMapping/fnom:functionParameter/fno:predicate ?param_map ;\n",
    "                       fno:parameterMapping/fnom:functionParameter/rdfs:label ?param_desc ;\n",
    "                       fno:returnMapping/fnom:functionOutput/fno:predicate ?return_map ;\n",
    "                       fno:returnMapping/fnom:functionOutput/rdfs:label ?return_desc .\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df = graph_manager.query(\n",
    "            regex_add_strings(\n",
    "                SPARQL_query,\n",
    "                map_id = map_id\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return df.groupby(['imp']).agg(\n",
    "            {\n",
    "                'function': lambda x: x.tolist(),\n",
    "                'param_map': lambda x: x.tolist(),\n",
    "                'param_desc': lambda x: x.tolist(),\n",
    "                'return_map': lambda x: x.tolist(),\n",
    "                'return_desc': lambda x: x.tolist()\n",
    "            }\n",
    "        ).reset_index().to_dict(orient='records')[0]\n",
    "        \n",
    "    def get_sparql_results(self, \n",
    "                           sub_question: str, \n",
    "                           sparql_query: str, \n",
    "                           parameter_info:str,\n",
    "                           parameter_descriptions: str,\n",
    "                           atomic_question: str\n",
    "                           ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes a SPARQL query and returns the results as a DataFrame.\n",
    "        R: get_sparql_results\n",
    "        \"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert SPARQL query executor. given a SPARQL query Templete and question infomation,\n",
    "        you will fill in the template and provide a executable sparql query. \n",
    "        \n",
    "        ONLY return the SPARQL query:\n",
    "        \n",
    "        #class\n",
    "        {class_schema}  \n",
    "        \"\"\"\n",
    "        \n",
    "        system_prompt = regex_add_strings(\n",
    "            system_prompt,\n",
    "            class_schema = \"\\n\".join([f\"{k}: {v['description']}\" for k, v in self.definitions['class_definitions'].items()])\n",
    "            )\n",
    "        \n",
    "        user_prompt = \"\"\"\n",
    "        ### Step Information:\n",
    "        \n",
    "        Main Question: {main_question}\n",
    "        Step Sub-Question: {sub_question}\n",
    "        Step Atomic Question: {atomic_question}\n",
    "        \n",
    "        parameters: {parameters}\n",
    "        parameter descriptions: {parameter_desc}\n",
    "        SPARQL Query Template: {sparql_query}\n",
    "        \n",
    "        SPARQL QUERY:\n",
    "        \"\"\"\n",
    "        \n",
    "        user_prompt = regex_add_strings(\n",
    "            user_prompt,\n",
    "            main_question = self.main_question,\n",
    "            sub_question = sub_question,\n",
    "            atomic_question = atomic_question,\n",
    "            parameters = parameter_info,\n",
    "            parameter_desc = parameter_descriptions,\n",
    "            sparql_query = sparql_query\n",
    "        )\n",
    "        \n",
    "        query = llm_chat(system_prompt, user_prompt, 'gpt-4o')\n",
    "        query = extract_sparql(query)\n",
    "        df = graph_manager.query(query)\n",
    "        return df\n",
    "    \n",
    "    def get_llm_results(self, sub_question: str, prev_results: List[Dict[str, Any]]) -> Any:\n",
    "        \"\"\"\n",
    "        Uses an LLM to process previous results and answer the sub-question.\n",
    "        R: get_llm_results\n",
    "        \"\"\"\n",
    "        _result_str = \"\"\n",
    "        for prev_result in prev_results:\n",
    "            _result_str += f\"Sub-question: {prev_result['sub_question']}\\n\"\n",
    "            _result_str += f\"Results: {json.dumps(prev_result['results'], indent=2)}\\n\\n\"\n",
    "            \n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert question answering agent. given previous results from knowledge graph queries,\n",
    "        you will process them to answer the sub-question and also provide the entities important to answer next step.\n",
    "        \n",
    "        Give the answer in JSON format with two fields:\n",
    "        - answer: The answer to the sub-question.\n",
    "        - important_entities: A list of important entities to consider for the next step.  \n",
    "        \"\"\"\n",
    "        \n",
    "        user_prompt = \"\"\"\n",
    "        ### Step Information:\n",
    "        Main Question: {main_question}\n",
    "        \n",
    "        #### Previous results:\n",
    "        {prev_results_text}\n",
    "        \n",
    "        Step Sub-Question: {sub_question}  \n",
    "        \n",
    "        Results:\n",
    "        \"\"\"\n",
    "        \n",
    "        user_prompt = regex_add_strings(\n",
    "            user_prompt,\n",
    "            main_question = self.main_question,\n",
    "            prev_results_text = _result_str,\n",
    "            sub_question = sub_question\n",
    "        )\n",
    "        \n",
    "        response = llm_chat(system_prompt, user_prompt, 'gpt-4o')\n",
    "        response = return_json_formatted(response)\n",
    "        return response\n",
    "        \n",
    "\n",
    "    def get_llm_results_sparql(self, sub_question: str, prev_results: List[Dict[str, Any]], sparql_results: pd.DataFrame) -> Any:\n",
    "        \"\"\"\n",
    "        Uses an LLM to process previous results and answer the sub-question.\n",
    "        R: get_llm_results\n",
    "        \"\"\"\n",
    "        _result_str = \"\"\n",
    "        for prev_result in prev_results:\n",
    "            _result_str += f\"Sub-question: {prev_result['sub_question']}\\n\"\n",
    "            _result_str += f\"Results: {json.dumps(prev_result['results'], indent=2)}\\n\\n\"\n",
    "            \n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert question answering agent. given previous results from knowledge graph queries,\n",
    "        you will process them to answer the sub-question.  you are given also the SPARQL results from the current step \n",
    "        to help you answer the sub-question and also provide the entities important to answer next step.\n",
    "        \n",
    "        Give the answer in JSON format with two fields:\n",
    "        - answer: The answer to the sub-question.\n",
    "        - important_entities: A list of important entities to consider for the next step.\n",
    "        \"\"\"\n",
    "        \n",
    "        user_prompt = \"\"\"\n",
    "        ### Step Information:\n",
    "        Main Question: {main_question}\n",
    "        \n",
    "        #### Previous results:\n",
    "        {prev_results_text}\n",
    "        \n",
    "        Step Sub-Question: {sub_question} \n",
    "        Step SPARQL Results: {sparql_results} \n",
    "        \n",
    "        Results:\n",
    "        \"\"\"\n",
    "        \n",
    "        user_prompt = regex_add_strings(\n",
    "            user_prompt,\n",
    "            main_question = self.main_question,\n",
    "            prev_results_text = _result_str,\n",
    "            sub_question = sub_question,\n",
    "            sparql_results = sparql_results.to_string()\n",
    "        )\n",
    "        \n",
    "        response = llm_chat(system_prompt, user_prompt, 'gpt-4o')\n",
    "        response = return_json_formatted(response)\n",
    "        return response\n",
    "    \n",
    "    def execute_step(self, step_name: str, prev_results: List[Dict[str, Any]]) -> Any:\n",
    "        \"\"\"\n",
    "        Executes a single step of the plan.\n",
    "        R: execute_step\n",
    "        \"\"\"\n",
    "        self.logs[step_name] = {}\n",
    "        step = self.plan.get(step_name, {})\n",
    "        sub_question = step[\"sub-question\"] # type: ignore\n",
    "        \n",
    "        atomic_question = step[\"used_atomic_question\"] # type: ignore\n",
    "        \n",
    "        if atomic_question.lower() == \"null\":\n",
    "            atomic_question = \"null\"\n",
    "\n",
    "            if len(atomic_question.split(\"|\")) < 3:\n",
    "                atomic_question = atomic_question.split(\"|\")\n",
    "                atomic_question = atomic_question[0]\n",
    "\n",
    "            atomic_question = string_closest_match(atomic_question, ques_info['question_lbl'].tolist())\n",
    "            ques_info_row = ques_info[ques_info['question_lbl'] == atomic_question]\n",
    "            mapping_id = ques_info_row['mapping'].values[0]\n",
    "            \n",
    "            self.logs[step_name]['mapping_id'] = mapping_id\n",
    "            sparql_query_info = self.get_function_sparql(mapping_id)\n",
    "            self.logs[step_name]['sparql_query_info'] = sparql_query_info\n",
    "            \n",
    "            _results = self.get_sparql_results(\n",
    "                sub_question,\n",
    "                sparql_query_info['imp'],\n",
    "                sparql_query_info['param_map'],\n",
    "                sparql_query_info['param_desc'],\n",
    "                atomic_question\n",
    "            )\n",
    "            \n",
    "            self.logs[step_name]['sparql_results'] = _results.to_dict(orient='records')\n",
    "            \n",
    "            # Use LLM to process previous results and current SPARQL results\n",
    "            _results = self.get_llm_results_sparql(\n",
    "                sub_question, \n",
    "                prev_results, \n",
    "                _results\n",
    "                )\n",
    "        else:\n",
    "            # Use LLM to process previous results\n",
    "            _results = self.get_llm_results(\n",
    "                sub_question, \n",
    "                prev_results)\n",
    "            \n",
    "        return {\"sub_question\": sub_question, \"results\": _results}\n",
    "    \n",
    "    def execute_plan(self) -> Any:\n",
    "        \"\"\"\n",
    "        Executes the plan of execution.\n",
    "        R: execute_plan\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for step_name, _ in self.plan.items():\n",
    "            try:\n",
    "                step_result = self.execute_step(step_name, prev_results=results)\n",
    "                results.append(step_result)\n",
    "            except Exception as e:\n",
    "                log.error(f\"Error executing step {step_name}: {e}\")\n",
    "            pass\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    def save_logs(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Saves the logs to a JSON file.\n",
    "        R: save_logs\n",
    "        \"\"\"\n",
    "        _dir, _ = os.path.split(filepath)\n",
    "        if not os.path.exists(_dir):\n",
    "            os.makedirs(_dir)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.logs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93356bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dome = common_utils.serialization.load_json(\"./logs/question_breakdown_user_question_20251115173101.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b510bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dome = {\"question\":\"what are the programs generated by AI?\", \"plan\":dome['best_plan']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb764a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
